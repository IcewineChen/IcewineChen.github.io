<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Hysteria</title>
    <link>http://yoursite.com/</link>
    <atom:link href="/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>Keep Alive</description>
    <pubDate>Sun, 23 Sep 2018 12:56:57 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>教师论</title>
      <link>http://yoursite.com/2018/09/23/%E6%95%99%E5%B8%88%E8%AE%BA/</link>
      <guid>http://yoursite.com/2018/09/23/%E6%95%99%E5%B8%88%E8%AE%BA/</guid>
      <pubDate>Sun, 23 Sep 2018 12:54:03 GMT</pubDate>
      <description>
      
        
        
          &lt;p&gt;缺德请挨打。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.mysupervisor.org/viewtopic.php?f=1096&amp;amp;t=50671&amp;amp;sid=18f3512f1b806b030a741a6746af0969&quot; target=&quot;_b
        
      
      </description>
      
      <content:encoded><![CDATA[<p>缺德请挨打。</p><p><a href="https://www.mysupervisor.org/viewtopic.php?f=1096&amp;t=50671&amp;sid=18f3512f1b806b030a741a6746af0969" target="_blank" rel="noopener">https://www.mysupervisor.org/viewtopic.php?f=1096&amp;t=50671&amp;sid=18f3512f1b806b030a741a6746af0969</a></p>]]></content:encoded>
      
      <comments>http://yoursite.com/2018/09/23/%E6%95%99%E5%B8%88%E8%AE%BA/#disqus_thread</comments>
    </item>
    
    <item>
      <title>生为圣人</title>
      <link>http://yoursite.com/2018/09/23/photo/</link>
      <guid>http://yoursite.com/2018/09/23/photo/</guid>
      <pubDate>Sun, 23 Sep 2018 12:37:32 GMT</pubDate>
      <description>
      
        
        
          &lt;p&gt;想活的像个圣人，奈何没人给个机会。&lt;/p&gt;

        
      
      </description>
      
      <content:encoded><![CDATA[<p>想活的像个圣人，奈何没人给个机会。</p>]]></content:encoded>
      
      <comments>http://yoursite.com/2018/09/23/photo/#disqus_thread</comments>
    </item>
    
    <item>
      <title>训练杂谈</title>
      <link>http://yoursite.com/2018/09/15/medicine/</link>
      <guid>http://yoursite.com/2018/09/15/medicine/</guid>
      <pubDate>Sat, 15 Sep 2018 02:39:06 GMT</pubDate>
      <description>
      
        &lt;h2 id=&quot;dl杂谈&quot;&gt;&lt;a href=&quot;#dl杂谈&quot; class=&quot;headerlink&quot; title=&quot;dl杂谈&quot;&gt;&lt;/a&gt;dl杂谈&lt;/h2&gt;&lt;p&gt;其实今年的文章总体有这样一种趋势，除了一些结构上具有开创意义的创新，大家都是小修小补，想发文章就找个差点的baseline飚一飚，比赛就找最好的部件拼一拼开始加trick开始大炼丹。其实都无外乎网络加深提feature再大batch到小batch交替摩擦。loss不降？查查数据，没问题加几层。上大batch加lr再小batch磨一磨，反正有BN这种东西以后大batch满天飞。所以说啥炼丹心得可说还真不容易总结。在这也就是随便一聊。&lt;/p&gt;
&lt;p&gt;不过这倒是可以先谈个问题：你train的真的准么……为啥我问这个问题？你可以自己手写几个层，跟pytorch和tf封装好的层测测输出误差……其实很好奇，为啥pytorch调的cudnn，误差居然比tf大。我有点没搞懂到底谁有问题了。最神奇的还是跟手写的卷积都不一样……如果测试没问题，那还是tensorflow的更准一点，不排除因为浮点精度问题出现了累积误差，但是这确实也挺难搞懂的，回头有时间仔细看看pytorch的卷积怎么写的。&lt;/p&gt;
&lt;h2 id=&quot;training技巧&quot;&gt;&lt;a href=&quot;#training技巧&quot; class=&quot;headerlink&quot; title=&quot;training技巧&quot;&gt;&lt;/a&gt;training技巧&lt;/h2&gt;&lt;p&gt;其实就实验科学的结果来说，这东西真的是个玄学。&lt;br&gt;
      
      </description>
      
      <content:encoded><![CDATA[<h2 id="dl杂谈"><a href="#dl杂谈" class="headerlink" title="dl杂谈"></a>dl杂谈</h2><p>其实今年的文章总体有这样一种趋势，除了一些结构上具有开创意义的创新，大家都是小修小补，想发文章就找个差点的baseline飚一飚，比赛就找最好的部件拼一拼开始加trick开始大炼丹。其实都无外乎网络加深提feature再大batch到小batch交替摩擦。loss不降？查查数据，没问题加几层。上大batch加lr再小batch磨一磨，反正有BN这种东西以后大batch满天飞。所以说啥炼丹心得可说还真不容易总结。在这也就是随便一聊。</p><p>不过这倒是可以先谈个问题：你train的真的准么……为啥我问这个问题？你可以自己手写几个层，跟pytorch和tf封装好的层测测输出误差……其实很好奇，为啥pytorch调的cudnn，误差居然比tf大。我有点没搞懂到底谁有问题了。最神奇的还是跟手写的卷积都不一样……如果测试没问题，那还是tensorflow的更准一点，不排除因为浮点精度问题出现了累积误差，但是这确实也挺难搞懂的，回头有时间仔细看看pytorch的卷积怎么写的。</p><h2 id="training技巧"><a href="#training技巧" class="headerlink" title="training技巧"></a>training技巧</h2><p>其实就实验科学的结果来说，这东西真的是个玄学。<br><a id="more"></a><br>下面先讲讲我自己的实验习惯，解释的话可能过段时间再详细开一篇聊聊。 首先我们讲一讲选模型的原则。首先如果需要深层次的特征，抛开任务，提特征的时候有人都会偏爱直接选择resnet之类的比较深比较大的网络，但是很多时候在试验的情况下发现loss一直不降，震荡，不收敛，这时候……先看看数据集或是喂的数据有没有问题才是第一要务。先检查自己prepare的数据是否没问题，比如检测自己的crop是不是有问题，或是在其他的一些数据处理函数上可视化view一下。</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">如果本身喂进网络的数据没有问题，检查一下数据集。如果样本数量本来就不多，可以考虑先用类似的小一点模型，比如resneXt置换成GoogLenet，设置好超参数。</span><br><span class="line">如果在较小的网络上这组超参可以收敛，再把这组参数迁移到大模型上，如果还是不收敛或是还在震荡，可以设optimizer为adam，调小一点<span class="keyword">batch</span>，然后看看能不能在训练中跳出局部极值。跳出去以后可以再选用大<span class="keyword">batch</span> size和之前在小模型上使用的超参继续训。</span><br></pre></td></tr></table></figure><p>说到这可能有入坑小伙伴还没搞懂大batch到底有个啥用，或者batch size这个参数到底影响了什么……其实引入batch normalization后，确实大batch带来的收益更诱人了，目前大量的网络结构也加了相当多的bn层，这样使用大batch有了很好的条件。在这我先简单解释下batch size的一些作用或者意义：</p><ul><li>首先我们谈谈大batch的作用。以sgd举例，事实上sgd大家都知道选一批样本去更新梯度，学这个梯度下降的方向。我们选用大batch，样本更有代表性，因为选择的样本越多，下降肯定越趋于整个数据集收敛应该朝向的方向，所以大batch会更稳定，基本不会在局部震荡， 因为每次参数更新所用到的数据越多，越能代表整体损失函数的梯度，因此梯度精确度更高。另一点是快，而且可以更好利用矩阵计算库里大矩阵乘法的效率，还可以提高显存利用率。</li><li>那小batch自然没有上述的好处，取一个极端例子，如果我们把batch size设为1，这样称为在线学习，每次修正方向以各自样本的梯度方向修正，难以达到收敛。可以理解为因为每次梯度更新只用一个样本，所以后果就是很可能在某一个局部极值点附近震荡……同理可辐射到其他小batch size上，但是同样也有好处，可以在达到一定程度后用来精细的去磨一下你的模型。因为小批量在学习过程中加入了噪声，会有一些正则化效果，但是需要小的学习率保持稳定性，否则还是会跳进局部极值难以自拔</li><li>对于大batch size，达到相同精度需要更多epoch，因为每轮训练中的迭代次数更少。</li></ul><p>所以可以总结下，适当增大batch size好处在于：</p><ul><li>内存利用率提高了。</li><li>对于相同数据量的处理速度更快。</li><li>在一定范围内，一般来说batch size 越大，其确定的下降方向越准，引起训练震荡越小。</li></ul><p>但是盲目增大绝对不行，毕竟首先不是所有实验室都有那么好的设备，batch size上去了，首先也得有足够内存和显存支撑。 另外根据上面提到的要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢。主要还有一点，batch size大到一定程度，下降方向已经基本不再变化，加了也没什么用。 所以说本质上还是需要考虑到你的任务到底塞多少合适，另外你的设备到底能怎么撑。一般来说，取2,4,8这么多块卡并行跑，选择一个合适的batch size塞显存就很重要了，因为要考虑显存占用率，你的实验室基友们也不是不跑实验的……不过一般来说，开始训勇敢取个大batch并没啥坏处，有bn撑腰，一切显得美好很多。</p><p>另一方面普遍要动的超参就是学习率了。嗯……这没啥好说的，为了训练稳定自然都愿意选小一点的lr或是先大lr降，不收敛不稳定再换小lr跳出震荡。其他自引入超参，emmmm，仁者见仁智者见智，其实是闻者伤心听者流泪……都是调参血泪史。</p><p>当然了，上面说的一切都要看你的实验室/公司项目组有没有钱，毕竟钱才是科学第一推动力。感谢以前搭集群的师兄们，真的，实属带善人。</p>]]></content:encoded>
      
      <comments>http://yoursite.com/2018/09/15/medicine/#disqus_thread</comments>
    </item>
    
    <item>
      <title>反向飞驰</title>
      <link>http://yoursite.com/2018/09/09/run-backward/</link>
      <guid>http://yoursite.com/2018/09/09/run-backward/</guid>
      <pubDate>Sun, 09 Sep 2018 15:32:23 GMT</pubDate>
      <description>
      
        
        
          &lt;p&gt;最近事情堆积如山，准备cvpr的进一步实验，开始校招的长周期准备。其实投稿大概就是督促自己做点事情，毕竟一直只是实验不带着压力工作效率就不高。&lt;/p&gt;
&lt;p&gt;前途非常迷，今年校招的行情比较迷，看不懂，没有一篇顶会文章算法岗做敲门砖，算法岗很难进大厂，本身我在研究上也不擅长。
        
      
      </description>
      
      <content:encoded><![CDATA[<p>最近事情堆积如山，准备cvpr的进一步实验，开始校招的长周期准备。其实投稿大概就是督促自己做点事情，毕竟一直只是实验不带着压力工作效率就不高。</p><p>前途非常迷，今年校招的行情比较迷，看不懂，没有一篇顶会文章算法岗做敲门砖，算法岗很难进大厂，本身我在研究上也不擅长。准备拿起老本行，准备找开发，负面情绪堆积严重，毕竟和以前赶上好时候的人相比容易心态爆炸，没有比较就没有伤害。</p><p>其实大部分时间我对于成功和开心的定义都比别人简单：</p><ul><li>波澜不惊就算开心</li><li>比普通人过的好一点就算成功</li></ul><p>然而最近基本都是反向飞驰。没有一天安稳，没有一天看到超越平庸的曙光。</p><p>急需调整心态，明天开始扑在framework修改和刷题。</p>]]></content:encoded>
      
      <comments>http://yoursite.com/2018/09/09/run-backward/#disqus_thread</comments>
    </item>
    
    <item>
      <title>pytorch升级和imread区别</title>
      <link>http://yoursite.com/2018/09/01/iocv/</link>
      <guid>http://yoursite.com/2018/09/01/iocv/</guid>
      <pubDate>Sat, 01 Sep 2018 08:05:33 GMT</pubDate>
      <description>
      
        &lt;p&gt;前段时间写了一版multiposenet的结构，调试的时候发现了几个问题。&lt;/p&gt;
&lt;p&gt;首先是pytorch0.4.0到0.4.1的更新。这个版本的更新里引入了一个比较有用的降采样函数，torch.nn.Functional.interpolate。文档上介绍的也很明确，调用时可以指定升/降采样的size或者一个尺度因子scale_factor。之前其实一直没有一个官方给出的降采样函数或是层，原来融合多个size每次都要复用手写的层，某些时候还不知道对不对的上别人文章的降采样方法……这回添加了一个官方的降采样方法，这样可能细节上可能大家都能统一一点，实验代码也好写一点或是复现一点，而且可以下采样到任意size或是scale，避免了很多方法实现上的微妙区别。load模型的时候跟0.4.0没什么区别，不像0.3.0升0.4.0时由于upsample变动导致我大量模型load不能的囧境。&lt;/p&gt;
&lt;p&gt;另外就是关于skimage.io.imread和cv2.imread的一个细节问题，读灰度图时，skimage的读取会读出2通道，而opencv中不添加灰度的flag的时候传入会默认处理成三通道。一开始我随手拿io.imread读了原图，原灰度图是这个样子：&lt;br&gt;&lt;img src=&quot;http://wx1.sinaimg.cn/mw690/005uXRWzly1fuu5q3psxhj30bl09076u.jpg&quot; alt=&quot;avatar&quot;&gt;&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>前段时间写了一版multiposenet的结构，调试的时候发现了几个问题。</p><p>首先是pytorch0.4.0到0.4.1的更新。这个版本的更新里引入了一个比较有用的降采样函数，torch.nn.Functional.interpolate。文档上介绍的也很明确，调用时可以指定升/降采样的size或者一个尺度因子scale_factor。之前其实一直没有一个官方给出的降采样函数或是层，原来融合多个size每次都要复用手写的层，某些时候还不知道对不对的上别人文章的降采样方法……这回添加了一个官方的降采样方法，这样可能细节上可能大家都能统一一点，实验代码也好写一点或是复现一点，而且可以下采样到任意size或是scale，避免了很多方法实现上的微妙区别。load模型的时候跟0.4.0没什么区别，不像0.3.0升0.4.0时由于upsample变动导致我大量模型load不能的囧境。</p><p>另外就是关于skimage.io.imread和cv2.imread的一个细节问题，读灰度图时，skimage的读取会读出2通道，而opencv中不添加灰度的flag的时候传入会默认处理成三通道。一开始我随手拿io.imread读了原图，原灰度图是这个样子：<br><img src="http://wx1.sinaimg.cn/mw690/005uXRWzly1fuu5q3psxhj30bl09076u.jpg" alt="avatar"></p><a id="more"></a><p>训练跑到一半的时候突然发现tensor运算的时候左右值维度不匹配，才发现了这个细节问题……之前一直都是用cv2.imread,也不会往里传grey的相关参数，直接默认读出三通道，但是灰度图直接拿io.imread调用时会出现shape只有(width,height)两位的问题。去翻了翻skimage的源码，as_grey参数已经deprecated了，其中有这么一段处理：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="selector-tag">img</span><span class="selector-class">.ndim</span> &gt; <span class="number">2</span>:</span><br><span class="line">    <span class="keyword">if</span> <span class="selector-tag">img</span><span class="selector-class">.shape</span>[-<span class="number">1</span>] not <span class="keyword">in</span> (<span class="number">3</span>, <span class="number">4</span>) and <span class="selector-tag">img</span><span class="selector-class">.shape</span>[-<span class="number">3</span>] <span class="keyword">in</span> (<span class="number">3</span>, <span class="number">4</span>):</span><br><span class="line">        <span class="selector-tag">img</span> = np.swapaxes(<span class="selector-tag">img</span>, -<span class="number">1</span>, -<span class="number">3</span>)</span><br><span class="line">        <span class="selector-tag">img</span> = np.swapaxes(<span class="selector-tag">img</span>, -<span class="number">2</span>, -<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> as_gray:</span><br><span class="line">        <span class="selector-tag">img</span> = rgb2gray(img)</span><br></pre></td></tr></table></figure></p><p>内部在ndim大于3的情况下，flag设为True也只是会粗暴的直接转化，ndim为2的时候他就直接读出(height,width)维的数组了，所以说这个确实……cv的读取就避免了这个问题。看来还好以前都是直接调cv.imread，这次因为要调transform顺手调了io，以后还是得注意数据喂进去到底是啥……以前也出现过喂错东西跑瞎实验的情况，毕竟数据驱动型的工作，数据的纯净度、形式组织还是至关重要的，得涨点记性。</p><p>另外就是multiposenet的子网络问题，目前我也不知道作者在test-dev上的统一size到底用了多大。而且按照原文中的结构，不给出一些细节，最后的ground truth和heatmap的size是绝对不匹配的，目前我强制采样到了256x256，有理解的朋友可以交流一下，代码repo在这：<a href="https://github.com/IcewineChen/pytorch-MultiPoseNet" target="_blank" rel="noopener">https://github.com/IcewineChen/pytorch-MultiPoseNet</a></p>]]></content:encoded>
      
      <comments>http://yoursite.com/2018/09/01/iocv/#disqus_thread</comments>
    </item>
    
  </channel>
</rss>
