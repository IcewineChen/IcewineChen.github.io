<!DOCTYPE html>
<html>
  <!-- Html Head Tag-->
  <head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="">
  <meta name="author" content="IcewineChen">
  <!-- Open Graph Data -->
  <meta property="og:title" content="pytorch-hg-3d代码分析"/>
  <meta property="og:description" content="Keep Calm" />
  <meta property="og:site_name" content="Hysteria"/>
  <meta property="og:type" content="article" />
  <meta property="og:image" content="http://yoursite.com"/>
  
    <link rel="alternate" href="/atom.xml" title="Hysteria" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  

  <!-- Site Title -->
  <title>Hysteria</title>

  <!-- Bootstrap CSS -->
  <link rel="stylesheet" href="/css/bootstrap.min.css">
  <!-- Custom CSS -->
  
  <link rel="stylesheet" href="/css/style.light.css">

  <!-- Google Analytics -->
  

</head>

  <body>
    <!-- Page Header -->


<header class="site-header header-background" style="background-image: url(/img/default-banner-dark.jpg)">
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <div class="page-title with-background-image">
          <p class="title">pytorch-hg-3d代码分析</p>
          <p class="subtitle"></p>
        </div>
        <div class="site-menu with-background-image">
          <ul>
            
              <li>
                <a href="/">
                  
                  Home
                  
                </a>
              </li>
            
              <li>
                <a href="/archives">
                  
                  Archives
                  
                </a>
              </li>
            
              <li>
                <a href="https://github.com/<your-github-username>">
                  
                  Github
                  
                </a>
              </li>
            
              <li>
                <a href="mailto:<your-email-address>">
                  
                  Email
                  
                </a>
              </li>
            
          </ul>
        </div>
      </div>
    </div>
  </div>
</header>

<article>
  <div class="container typo">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <div class="post-info text-muted">
          
            <!-- Author -->
            <span class="author info">By IcewineChen</span>
          
          <!-- Date -->
          <span class="date-time info">On
            <span class="date">2018-07-16</span>
            <span class="time">00:10:58</span>
          </span>
          
        </div>
        <!-- Tags -->
        
          <div class="post-tags text-muted">
            Tags: 

<a class="tag" href="/tags/deep-learning/">#deep learning</a> <a class="tag" href="/tags/pytorch/">#pytorch</a> <a class="tag" href="/tags/human-pose-estimation/">#human pose estimation</a>


          </div>
        
        <!-- Post Main Content -->
        <div class="post-content">
          <p>上次组会分享了CVPR2018的3d pose估计文章 3D Human Pose Estimation in the Wild by Adversarial Learning,文章中提到一种用GAN的方式生成3D in the wild估计的方法。最近准备复现，顺带在这里先分析一下结构中Generator，顺带分析一下代码</p>
<p>生成器的结构参见Towards 3D Human Pose Estimation in the Wild: a Weakly-supervised Approach，感谢作者，该结构以stacked hourglass为base network做一个2d keypoint估计，然后扔进3d regression得到最后的结果。数据集选用了有3d ground truth和没有ground truth的in the wild 2d集拼合，整体结构上在2d估计中抽出了中间层信息送入回归器，迁移了in the wild的信息。</p>
<h2 id="pytorch网络代码分析"><a href="#pytorch网络代码分析" class="headerlink" title="pytorch网络代码分析"></a>pytorch网络代码分析</h2><p>先贴一段核心网络结构的代码：<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HourglassNet3D</span>(<span class="title">nn</span>.<span class="title">Module</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>, nStack, nModules, nFeats, nRegModules)</span></span><span class="symbol">:</span></span><br><span class="line">    <span class="keyword">super</span>(HourglassNet3D, <span class="keyword">self</span>).__init_<span class="number">_</span>()</span><br><span class="line">    <span class="keyword">self</span>.nStack = nStack</span><br><span class="line">    <span class="keyword">self</span>.nModules = nModules</span><br><span class="line">    <span class="keyword">self</span>.nFeats = nFeats</span><br><span class="line">    <span class="keyword">self</span>.nRegModules = nRegModules</span><br><span class="line">    <span class="keyword">self</span>.conv1<span class="number">_</span> = nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, bias = True, kernel_size = <span class="number">7</span>, stride = <span class="number">2</span>, padding = <span class="number">3</span>)</span><br><span class="line">    <span class="keyword">self</span>.bn1 = nn.BatchNorm2d(<span class="number">64</span>)</span><br><span class="line">    <span class="keyword">self</span>.relu = nn.ReLU(inplace = True)</span><br><span class="line">    <span class="keyword">self</span>.r1 = Residual(<span class="number">64</span>, int(<span class="number">128</span>))</span><br><span class="line">    <span class="keyword">self</span>.maxpool = nn.MaxPool2d(kernel_size = <span class="number">2</span>, stride = <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">self</span>.r4 = Residual(<span class="number">128</span>, <span class="number">128</span>)</span><br><span class="line">    <span class="keyword">self</span>.r5 = Residual(<span class="number">128</span>, <span class="keyword">self</span>.nFeats)</span><br><span class="line">    </span><br><span class="line">    _hourglass, _Residual, _lin<span class="number">_</span>, _tmpOut, _ll<span class="number">_</span>, _tmpOut<span class="number">_</span>, _reg<span class="number">_</span> = [], [], [], [], [], [], []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="keyword">self</span>.nStack)<span class="symbol">:</span></span><br><span class="line">      <span class="comment"># n阶的hourglass</span></span><br><span class="line">      _hourglass.append(Hourglass(<span class="number">4</span>, <span class="keyword">self</span>.nModules, <span class="keyword">self</span>.nFeats))</span><br><span class="line">      <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="keyword">self</span>.nModules)<span class="symbol">:</span></span><br><span class="line">      <span class="comment"># 连接stacks间的基本残差单元</span></span><br><span class="line">        _Residual.append(Residual(<span class="keyword">self</span>.nFeats, <span class="keyword">self</span>.nFeats))</span><br><span class="line">      lin = nn.Sequential(nn.Conv2d(<span class="keyword">self</span>.nFeats, <span class="keyword">self</span>.nFeats, bias = True, kernel_size = <span class="number">1</span>, stride = <span class="number">1</span>), </span><br><span class="line">                          nn.BatchNorm2d(<span class="keyword">self</span>.nFeats), <span class="keyword">self</span>.relu)</span><br><span class="line">      _lin<span class="number">_</span>.append(lin)</span><br><span class="line">      _tmpOut.append(nn.Conv2d(<span class="keyword">self</span>.nFeats, ref.nJoints, bias = True, kernel_size = <span class="number">1</span>, stride = <span class="number">1</span>))</span><br><span class="line">      _ll<span class="number">_</span>.append(nn.Conv2d(<span class="keyword">self</span>.nFeats, <span class="keyword">self</span>.nFeats, bias = True, kernel_size = <span class="number">1</span>, stride = <span class="number">1</span>))</span><br><span class="line">      _tmpOut<span class="number">_</span>.append(nn.Conv2d(ref.nJoints, <span class="keyword">self</span>.nFeats, bias = True, kernel_size = <span class="number">1</span>, stride = <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>)<span class="symbol">:</span></span><br><span class="line">      <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="keyword">self</span>.nRegModules)<span class="symbol">:</span></span><br><span class="line">        _reg<span class="number">_</span>.append(Residual(<span class="keyword">self</span>.nFeats, <span class="keyword">self</span>.nFeats))</span><br><span class="line">    <span class="comment"># append部分对应stackhourglass中的沙漏型结构，将其中用的residual模块和n个Stack整理进sequential的形式方便之后再循环里调用</span></span><br><span class="line">    <span class="comment"># modulelist的取法其实基本跟list一样，封装在modulelist里会检测到每一个的hook，直接包装在list里，每一个单元内部的参数会初始不到</span></span><br><span class="line">    <span class="keyword">self</span>.hourglass = nn.ModuleList(_hourglass)</span><br><span class="line">    <span class="keyword">self</span>.Residual = nn.ModuleList(_Residual)</span><br><span class="line">    <span class="keyword">self</span>.lin<span class="number">_</span> = nn.ModuleList(_lin<span class="number">_</span>)</span><br><span class="line">    <span class="keyword">self</span>.tmpOut = nn.ModuleList(_tmpOut)</span><br><span class="line">    <span class="keyword">self</span>.ll<span class="number">_</span> = nn.ModuleList(_ll<span class="number">_</span>)</span><br><span class="line">    <span class="keyword">self</span>.tmpOut<span class="number">_</span> = nn.ModuleList(_tmpOut<span class="number">_</span>)</span><br><span class="line">    <span class="keyword">self</span>.reg<span class="number">_</span> = nn.ModuleList(_reg<span class="number">_</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">self</span>.reg = nn.Linear(<span class="number">4</span> * <span class="number">4</span> * <span class="keyword">self</span>.nFeats, ref.nJoints)</span><br><span class="line">    </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(<span class="keyword">self</span>, x)</span></span><span class="symbol">:</span></span><br><span class="line">    <span class="comment"># 将定义了的层按顺序传递，构建整个网络的forward顺序</span></span><br><span class="line">    x = <span class="keyword">self</span>.conv1<span class="number">_</span>(x)</span><br><span class="line">    x = <span class="keyword">self</span>.bn1(x)</span><br><span class="line">    x = <span class="keyword">self</span>.relu(x)</span><br><span class="line">    x = <span class="keyword">self</span>.r1(x)</span><br><span class="line">    x = <span class="keyword">self</span>.maxpool(x)</span><br><span class="line">    x = <span class="keyword">self</span>.r4(x)</span><br><span class="line">    x = <span class="keyword">self</span>.r5(x)</span><br><span class="line">    </span><br><span class="line">    out = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="keyword">self</span>.nStack)<span class="symbol">:</span></span><br><span class="line">      hg = <span class="keyword">self</span>.hourglass[i](x)</span><br><span class="line">      ll = hg</span><br><span class="line">      <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="keyword">self</span>.nModules)<span class="symbol">:</span></span><br><span class="line">        ll = <span class="keyword">self</span>.Residual[i * <span class="keyword">self</span>.nModules + j](ll)</span><br><span class="line">      ll = <span class="keyword">self</span>.lin<span class="number">_</span>[i](ll)</span><br><span class="line">      tmpOut = <span class="keyword">self</span>.tmpOut[i](ll)</span><br><span class="line">      out.append(tmpOut)</span><br><span class="line">      </span><br><span class="line">      ll<span class="number">_</span> = <span class="keyword">self</span>.ll<span class="number">_</span>[i](ll)</span><br><span class="line">      tmpOut<span class="number">_</span> = <span class="keyword">self</span>.tmpOut<span class="number">_</span>[i](tmpOut)</span><br><span class="line">      <span class="comment"># 取多层信息，作为下一个hourglass的输入，也保存信息到送入reg的x里</span></span><br><span class="line">      x = x + ll<span class="number">_</span> + tmpOut<span class="number">_</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>)<span class="symbol">:</span></span><br><span class="line">      <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="keyword">self</span>.nRegModules)<span class="symbol">:</span></span><br><span class="line">        x = <span class="keyword">self</span>.reg<span class="number">_</span>[i * <span class="keyword">self</span>.nRegModules + j](x)</span><br><span class="line">      x = <span class="keyword">self</span>.maxpool(x)</span><br><span class="line">      </span><br><span class="line">    x = x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">    reg = <span class="keyword">self</span>.reg(x)</span><br><span class="line">    out.append(reg)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure></p>
<p>这就是3d stacked hourglass结构的代码。关于一个nn.Module的代码怎么写，其实参照pytorch源码的Module写就可以，由于自动微分这种东西的存在，继承Module时需要只定义init和forward函数即可。在init函数中，我们定义需要的层，需要的结构，然后在forward中，定义层的连接，也就是网络的结构。这段代码描述了生成器的结构，总体来说，nStack就是单个stacked hourglass的阶数，modules是使用级联的模块数，residual是一个残差结构。ll是我们一开始抽出来的stacked的输出层和中间层的和，也就是抽出层再加上输出的结果，之后在输入维度上做整合。reg是3d信息回归器的结构。在forward中，按照stacked hourglass的方法先加了conv+bn+relu+残差+max pooling的结构调整网络输入，之后搭建2d的级联结构，同时把每个stacked结构中的中间一层抽出来,和每个级联的stack一起送去3d regression。具体的代码注释和一些修改我都放在了我fork的repository里，欢迎来看整个注释和代码啊，现在还没彻底完成，还在这个的基础上加入GAN部分的代码（主要是multi-source的鉴别器）去复现GAN-hg-3d。准备在这个结构上调整，看看in the wild的3d pose的效果如何。大概觉得这个方向在工业界落地上可能还有点价值……以前实习的时候组里也有人做过2d的gesture，问题本质上都差不多。理论上行为这一块市场很大，pose的工作还是很有价值的。</p>
<p>pytorch在大部分时间是不需要定义backward的，基本上直接继承nn.Module就可以实现。不过有时候需要我们自定义loss，这时候loss从function里继承，这里其实本质上你也可以直接写在新的Module里，但是你可以尝试一下在Module的backward里面加上一些print，之后从外部调一下试试看。这时候是不会执行的。因为本身Module底层就是要进行backward的操作，每一步操作的时候都会生成一个Function的子类，最后把这些子类的forward和backward连接到一起。所以这些时候你在module内部写一个backward它根本就不会执行，因为实际上真的调用的是基于Function的backward操作。所以如果有大量自定义而pytorch的底层又没有实现的计算，可以直接自定义一个继承自Function的class，然后再用module封装进去。这也是官方文档的做法，在neural style transfer的那篇tutorial里很好的表示了loss的写法，想再自己动手试试看看的话可以去看一这篇文档。根据我的经验你大概只需要记住：绝大部分时候你不需要自己写backward，但是比如你要实现一种新的下降算法或是已经提供的计算不能满足的层定义，那再自己去写。</p>
<p>那么如果自定义一些操作，请遵照一下结构：<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomLoss</span>(<span class="title">torch</span>.<span class="title">autograd</span>.<span class="title">Function</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>,*kwargs)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">super</span>(CustomLoss,<span class="keyword">self</span>).__init_<span class="number">_</span>()</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(<span class="keyword">self</span>,*kwargs)</span></span><span class="symbol">:</span></span><br><span class="line">        raise NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(<span class="keyword">self</span>,)</span></span></span><br><span class="line">        raise NotImplementedError</span><br></pre></td></tr></table></figure></p>
<p>按照function继承是要定义backward的，对grad要进行底层实现，基本上只有loss这部分的定义需要费点事。mxnet的定义倒是可以根据符号编程自动推导，符号式编程加自动微分确实很方便，不过sym的调试其实并没有pytorch的动态图来的直观。感觉mxnet在复现上还是更简单快捷粗暴，不过文档不全和有些地方因为更新太快有bug，稍微有点脱了后腿。pytorch这次的更新据说填了不少坑,就我的感受来说，内存不那么容易被多输入Variable的索引跑到炸了，之前的话以Variable对接dataloader在梯度更新后经常要注意取loss[0]，否则直接会加入Variable中的各种参数。据说目前版本解决了这个问题。</p>
<p>数据集接口没什么太多问题的，dataloader只需要注意cv读和dataloader要求的维度顺序，还有定义最后混合的fusion结构中要构建包含2d ground truth和3d ground truth的样本即可。getitem基本不需要做多少改动，只需要按顺序，3d+2d顺序append，之后索引在3d样本数内读3d，大于部分get 2d样本即可。</p>
<h2 id="关于版本更新导致的问题"><a href="#关于版本更新导致的问题" class="headerlink" title="关于版本更新导致的问题"></a>关于版本更新导致的问题</h2><p>其实原repo的作者是给出了pretrain的模型的，但是在Stage2阶段是用不了的……我单点打到这一行，发现load的模型是有问题的，这一行定在upsample层上：</p>
<blockquote>
<p>self.up2 = nn.Upsample(scale_factor = 2)</p>
</blockquote>
<p>事实上upsample需要一个align_corners的参数，在0.4.0之前这个参数应该被直接给了None，导致在0.4.0版本load pretrain的模型后，跑到upsample层的forward时在module的检查时一直报错，会一直报给你检查不到align_corners。再打进内部检验的断点时，发现确实load的模型align_corner是None。如果使用我修改后的pytorch0.4.0兼容的代码train的模型是不存在这个问题的，然而我检查的时候依然没有找到align_corner参数，理论上应该default是False。这可能是底层实现有改动，更底层的部分我就step out没管了。这个大概是版本更新导致的bug，具体有待继续调试验证。如果想看pytorch修改后能用的代码，可以直接跑我fork后的fixed branch，里面已经做了对应的各种修改，在python2.7+ubuntu16.04+pytorch0.4.0下可以正常运行。</p>
<h2 id="后续工作"><a href="#后续工作" class="headerlink" title="后续工作"></a>后续工作</h2><p>之后会打算对文中内容复现。不过目前遇到一个问题是关节点depth map怎么得到。这个问题目前没找到什么合适的代码或是工具，如果有大佬知道的话希望能邮件或是github issue给我……非常感谢</p>

        </div>
      </div>
    </div>
  </div>
</article>



    <!-- Footer -->
<footer>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <p class="copyright text-muted">
          Theme By <a target="_blank" href="https://github.com/levblanc">Levblanc.</a>
          Inspired By <a target="_blank" href="https://github.com/klugjo/hexo-theme-clean-blog">Clean Blog.</a>
        <p class="copyright text-muted">
          Powered By <a target="_blank" href="https://hexo.io/">Hexo.</a>
        </p>
      </div>
    </div>
  </div>
</footer>


    <!-- After Footer Scripts -->
<script src="/js/highlight.pack.js"></script>
<script>
  document.addEventListener("DOMContentLoaded", function(event) {
    var codeBlocks = Array.prototype.slice.call(document.getElementsByTagName('pre'))
    codeBlocks.forEach(function(block, index) {
      hljs.highlightBlock(block);
    });
  });
</script>

  </body>
</html>

