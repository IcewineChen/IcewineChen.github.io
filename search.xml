<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[c++ 并行设计]]></title>
    <url>%2F2018%2F12%2F16%2Fconcurrency%2F</url>
    <content type="text"><![CDATA[motivation最近接了套实时sdk的活……由于引入了工程量巨大的tensor库和人家打包好的算法库，不是很想看源码，但是黑箱的调用和测试过程中被三方库的线程和内存管理搞得非常烦躁。看看源码，于是乎顺便写一篇这个来清醒一下脑子，整理一下思路 头文件 名字就暴露了一切…… 关于线程管理关于调用匿名函数作为thread对象引入lambda表达式解决问题即可。直接引入匿名构造会被视作调用函数，引发初始化问题。比如std::thread _mythread(func())会被视为一个返回值为thread的名为_mythread的函数，传了一个func()的返回类型的参。正确方法是利用lambda表达式构建匿名函数:123std::thread _mythread([param list]&#123; dosomething();&#125;) 或者是直接利用标准对象初始化符号std::thread _mythread{func()} join和detachjoin的引入时间非常重要，线程池毕竟不是万能的……项目中引入了三方线程池，不确定是否安全或是性能提升，还需要测试。detach直接将线程从队列分离，不再被引用，时间同样很重要，否则有造成已经release掉的内存访问的风险。可以try/catch保证给join或是detach的位置确保join机制没问题 同一操作接受不同的输入比如要同时编辑多个文档，编辑文档的函数可以定义为docu(),可以在里面新引入thread,设置thread(docu,new document),这样不会引发线程安全问题，因为虽然同样操作，但是访问不同资源 传引用作参例如void update_data_for_widget(widget_id w,widget_data&amp; data);我们直接在调用时传实参是一个widget_data类型的实参，会传递一个拷贝，类型检验会报错，这时候需要传std::ref(实参)，这样传递的是变量的引用，而不是一个拷贝后的引用 move直接转移线程所有权，同样适用于线程敏感性的容器操作，比如构建一个thread vector。这种自动化构建流程是独立的,结果只会受到共同访问资源的影响,把所有创建的线程join即可1std::for_each(threads.begin(),threads.end(),std::mem_fn(&amp;std::thread::join)) 保证算法并行的泛型引入泛型，可以保证分割的任务构造函数用的类型连贯，避免出现类型不匹配造成的精度损失 线程识别线程识别类型是std::thread::id,可以直接对线程类实例调用get_id()方法获取。不过要注意，如果两个thread实例调用get_id()后相等，可能是同一线程，但是也可能都是空的线程。也可以更粗暴，直接一个个扔进std::hash这样的容器，用id作为无序容器的键值。 具体的输出结果是严格依赖于具体实现的，C++标准的唯一要求就是要保证ID比较结果相等的线程，必须有相同的输出。 资源访问：锁的操作简单来说，上锁保证访问资源不会被n个同时访问的修改申请导致共同访问区的资源内容混乱，这可能导致修改共享数据，导致不变量破坏后依然不稳定，无法保持不变量的形式，导致存储的数据结构永久损坏，最后程序崩溃。bug重灾区，请小心谨慎。 数据竞争并发的修稿一个独立对象（资源访问区的不变量）。也可以通过细心规划线程分配，保证不变量的稳定性从而实现无锁，但是很困难。暂时还没试过……但是无脑上mutex和lock也会存在死锁问题。 mutex利用std::mutex创建互斥量，调用lock()上锁。但是每次都lock unlock不符合RAII哲学，所以需要lock_guard()即可以靠调用其析构解决这个问题，析构时自己解锁即可。通常情况下，互斥量和要保护的数据放在一个类里，并且作为private量。一些需要考虑上锁互斥的函数同样放在该类里，所有类内访问数据的方法都应该上锁，符合oop的设计原则，而不是定义全局变量，避免引发各种奇怪的问题。 这里有一个问题：当其中一个成员函数返回的是保护数据的指针或引用时，会破坏对数据的保护。具有访问能力的指针或引用可以访问(并可能修改)被保护的数据，而不会被互斥锁限制。互斥量保护的数据需要对接口的设计相当谨慎，要确保互斥量能锁住任何对保护数据的访问，并且不留后门。只要没有成员函数通过返回值或者输出参数的形式向其调用者返回指向受保护数据的指针或引用，数据就是安全的。 接口内部的线程安全处理接口内部的条件竞争:首先需要确保操作中资源访问区在并发中是安全的，比如双链表删除节点中的不变量变化过程中，删除的节点以及其前后节点不能被并行访问，必须在增删node的情况下保护整个数据结构。 通用的解决方案是重新设计接口逻辑。简单的异常捕获会让函数在条件竞争为false时变得没有调用意义，需要通过改变设计逻辑避免这个问题。比如对于栈而言，同时可能有多个线程访问stack内的值，然后调用pop或者top，这样的问题很难定位，可以通过对pop/top()操作直接上锁控制访问。如果要避免内存限制导致的pop()值丢失，重载stack.pop()，重载之后再内部上锁后可以返回一个shared_ptr指向栈顶指针。 避免deadlock的一般方案不同互斥量选择相同顺序上锁。不过c++11标准库的lock和lock_guard(就是boost里引过来的)可以一次锁住多个互斥量。lock_guard的初始化:std::lock_guard lck_name(mutex x)，构造直接引入互斥量。lock函数直接lock(mutex x)锁住互斥量。不同于mutex.lock()和unlock()方法，标准库std::lock()函数调用时不需要对应unlock()。 另外，对同一互斥量在线程内多次上锁，互斥量类型要用recursive_mutex。 避免死锁的建议根据c++ concurrency的推荐，有以下几点需要注意： 避免嵌套锁如果每个线程都只上一把锁，就不会死锁。如果需要保护多个数据操作，选择std::lock对多个互斥量上锁。 持有锁的控制权的情况下调用未知操作未知操作可能包含线程锁，这样会违背上一点，因为会造成嵌套锁。 对资源访问的线程的所有操作以同样的顺序上锁比如对双链表的删除，同时会访问三个node，这时候需要在线程的定义上都用固定顺序上锁，比如b节点删除，A节点是b节点的front，c是b的next，这时候可以定义线程先锁住a才能访问b的锁，锁住b才能访问c的锁 使用层次锁使用层次锁，遵守高层次-&gt;低层次上锁的顺序才可以运行，否则直接抛出异常。虽然hierarchical_mutex没有标准库实现，但是只需要外部能通过优先级的getter访问到即可实现 接口设计比如stack的pop和top操作，这种设计很可能在多线程条件下出现一个问题：多个线程对象的操作在top访问时top没执行完，虽然应该每次访问的top值不一样，但是因为top没执行完还是会出现一样的值，逻辑错误，或者两次top同时导致想弹出的值没能存下来。这个问题是由于stack内部的接口设计导致的，所以改进只能重设接口关系。因此可以重载pop方法，让其弹出指向一个shared_ptr，用shared_ptr保存这个弹出值。 关于condition的调用定义一个condition_variable的示例,这个对象可以直接调用wait方法，之后等待某处的notify调用时唤醒。在调用这个操作的线程运行到wait时就判断等待条件，返回true时继续持有锁，返回false则线程进入阻塞，等到下次notify时再次检验条件 atomic的相关操作]]></content>
  </entry>
  <entry>
    <title><![CDATA[android-bungumi]]></title>
    <url>%2F2018%2F10%2F21%2Fandroid-bungumi%2F</url>
    <content type="text"><![CDATA[bangumi的官方移动端客户端实在是有点迷……于是乎最近准备搞一个第三方客户端。写的过程中有些时候由于api升级，和最早学习的时候变化不小，遇到各种问题……毕竟生疏，把遇到的一些小问题总结一下。 关于推送8.0版本中的notify需要跟channel绑定，否则会报错。关于注册一个通知：先注册一个manager，再注册一个channel，之后把notification的builder中加入channel，否则会报channel为null的错12345678910111213141516171819202122232425NotificationManager manager = (NotificationManager) getSystemService(NOTIFICATION_SERVICE);String CHANNEL_ID = "my_channel_01";CharSequence name = "my_channel";String Description = "This is my channel";int importance = NotificationManager.IMPORTANCE_HIGH;NotificationChannel mChannel = new NotificationChannel(CHANNEL_ID, name, importance);mChannel.setDescription(Description);mChannel.enableLights(true);mChannel.setLightColor(Color.RED);mChannel.enableVibration(true);mChannel.setVibrationPattern(new long[]&#123;100, 200, 300, 400, 500, 400, 300, 200, 400&#125;);mChannel.setShowBadge(false);manager.createNotificationChannel(mChannel);Notification notification = new NotificationCompat.Builder(this,CHANNEL_ID). setContentTitle("Content"). setContentText("here is something"). setWhen(System.currentTimeMillis()). setSmallIcon(R.mipmap.ic_launcher). setPriority(NotificationCompat.PRIORITY_MAX). setLargeIcon(BitmapFactory.decodeResource(getResources(),R.mipmap.ic_launcher)). setAutoCancel(true).build();manager.notify(1,notification); 关于startActivityForResult(int requestcode,int resultcode,Intent data)上一个活动的数据存在intent中，requestcode只是用于和start时的code比较，确认返回的是哪个活动 音视频播放释放音频是调用MediaPlayer.release() 释放视频调用VideoView.suspend() http轻量级框架OkHttp3,文档和samples很全，随时可以根据需要翻阅github上的文档 线程安全问题 子线程不能进行UI操作 测试后不知为啥，stringbuffer线程出现不安全情况，不知道啥情况，回头调试 OKhttp的get请求可以正常拿到字段 OKhttp GET流程：先建立client，之后建立一个request，调用Request.builder，之后构建response = client.newCall(request).execute(),得到get请求的响应POST用FormBody.builder添加数据，之后正常bulder中调用一个post()即可 异步调用 由于androidUI线程不安全，需要引入异步 异步由四个部分组成：Message（线程中传递的消息类）,Handler,MessageQueue（每个线程中只有一个）和Looper AsyncTask 便于在子线程返回一些ui操作结果 抽象类，使用时继承，构造中有三个泛型参数：Params,Progress(进度单位),Result 需要重写的方法：onPreExecute(),doInBackground() -&gt; 这个函数置入所有处理耗时请求及线程，但是依然不能进行UI操作，用于返回操作结果,onProgressUpdate(),onPostExecute()-&gt;根据返回的数据进行UI操作 Service内部放置所有需要在后台执行的逻辑，逻辑为了调用一个asynctask定义的任务而服务 material UI调support.v7.widget.toolbar。调用之后，调用setSupportActionBar方法设置一个toolbar。刚接触material UI各种组件的写法，觉得应该总结一下。 滑动菜单在布局文件中调用android.support.v4.widget.DrawerLayout组件，布局文件写好后，调用时需要创建一个DrawerLayout对象获取滑动布局，之后调用openDrawer(GravityCompat.START)渲染出滑动菜单 滑动菜单布局除了调用DrawerLayout，需要再建一个滑出菜单的header部分和内容item部分，然后在主活动中分别定义每一项点击后的响应逻辑，代码如下，对item栏调用setNavigationItemSelectedListener。之后重写内部的onNavigationItemSelected中switch每一个栏对应的响应123456789101112131415161718192021222324252627282930313233@Overrideprotected void onCreate(Bundle savedInstanceState)&#123; super.onCreate(savedInstanceState); setContentView(R.layout.activity_main); Toolbar toolbar = findViewById(R.id.toolbar); setSupportActionBar(toolbar); mDrawerLayout = findViewById(R.id.drawer_layout); navView = findViewById(R.id.nav_view); ActionBar actionBar = getSupportActionBar(); if (actionBar!=null)&#123; actionBar.setDisplayHomeAsUpEnabled(true); actionBar.setHomeAsUpIndicator(R.drawable.ic_menu); &#125; navView.setCheckedItem(R.id.nav_call); navView.setNavigationItemSelectedListener(new NavigationView.OnNavigationItemSelectedListener() &#123; @Override public boolean onNavigationItemSelected(@NonNull MenuItem item) &#123; mDrawerLayout.closeDrawers(); return true; &#125; &#125;);&#125;@Overridepublic boolean onOptionsItemSelected(MenuItem item) &#123; switch (item.getItemId())&#123; case android.R.id.home: mDrawerLayout.openDrawer(GravityCompat.START); break; default: &#125; return true;&#125; DrawerLayout在drawerLayout下，有两个子布局，其中第一个子布局定义背景，第二个子布局定义滑动栏的布局。背景布局相对复杂一点，一定记得选一个合适的布局模式。 关于布局中下拉刷新和下拉隐藏toolbar的方法下拉隐藏toolbar：1需要先解决toolbar和下面的布局的遮挡问题，这时候需要引入android.support.design.widget.AppBarLayout,在里面把toolbar加进去，之后把下面主要内容的布局中指定一个app:layout_behaviour="@string/appbar_scrolling_view_behaviour"，然后给toolbar中添加app:layout_scrollFlags="scroll|enterAlways|snap"属性]]></content>
  </entry>
  <entry>
    <title><![CDATA[2018番剧阶段总结]]></title>
    <url>%2F2018%2F10%2F03%2Fanimal%2F</url>
    <content type="text"><![CDATA[吹水趁着放假看了动物世界，怎么说呢，还行，挺好看，值得一看的电影，可以给个七分。李易峰风评可以逆转了，表现的确实不错。 那这跟番剧总结有啥关系啊……当然有啊，七月番管理利根川啊……看完利根川和动物世界，觉得最近可以补一下赌博默示录。 其实今年挺惨淡。热门原创几乎全扑街，一月京紫，国家队，FE三大热门全凉，四月大热拳击只能说不温不火吧，明日之丈压缩过份了，剧情对不起作画，七月更是没啥好说的，亲妈飞扬自闭羽毛球也是服的不行。在娘化少女系作品横飞的时代，今年还真就南极，露营沙滩打排球能看的，蛮神奇的。从部分值得说说的东西提起吧。 ditf这个我真是无话可说……超神的前半季，19集以前都可以接受，20话开始急转直下，看完以后我都不敢说我跟人推荐过这玩意。今石洋之+锦织敦史可以搞出天元突破，斩服少女这种东西，俩人都打下手可以出flcl这种好看的神经病作品，但是sb锦织+今石就能做出这种剧情四不像节奏秘制尴尬的东西，后期节奏简直迷得一塌糊涂，不知道的以为自己看了生育宣传片=-= 京紫没啥说的，京都大型作画宣传。我还是等明年京吹剧场吧。今年京都最好的作品大概是中二病剧场。京紫很好，就是不太对的起大家期待，自家文库质量确实不行，不如继续跟角川合作吧，不需要凉宫冰菓这种，再出一部轻音少女水平的作品就好了，京紫这带制作，真的可惜了。不是很推荐，但是补番优先级比国家队高。 FELE月球试验作品，其实我觉得还蛮不错的，了解一点背景去看，蛮有意思的，就是新房不该穿插那么多意识流，感觉还不错，就是分割放送有点尴尬，其实我觉得内涵主题和打斗表现都还可以。 排球露营南极三个放一起，因为通篇只有推荐。看着很舒服，不说多好，但是看起来真的很舒服，不费心思，尤其露营。都推荐看看。 石头门0都被吹爆了,事实上我没这感觉……唯一值得称道的可能就是第八集助手的短信圆了本篇的SG本篇的问题：助手到底发了啥？事实上SG0有个非常重要的问题：amadeus加戏有点多。嘟嘟噜的主场在前半段都快被amadeus打爆了。而且在36年的跳跃略去不少原作细节，这也没办法，但是我觉得角色定位和表达出了问题，花田写出这样的脚本没问题，原脚本林直孝难道不知道自己游戏里写了啥？八分可以给，再高真不行…… megalo box一开始吹爆，后续大概给8分吧。我觉得剧情和节奏都还好，就是打拳部分不够多。运动番我觉得现在陷入了一个问题，就是不知道主题在哪。如果走热血路线，多渲染赛场气氛；打青春路线，安达充的作品照着模板做就行了，但是megalo box有点想都做到好，结果各方面都不错，但都差一点。音乐是真不错，看是绝对值得看。 lost song + island怎么说呢，这种轮回系作品我觉得放一块说比较好。其实lost song我觉得兑现上限了，毕竟12集的季番，又是原创作品不能要求太多。总体很不错了，但是得耐住前七集那个非常无聊的门槛。 island怎么说呢……比cc动画强，和rewrite可以做朋友。我是有点失望，本来我很期待动画能做的好一点，虽然不可能。这种剧情悬疑型的gal改一般都死得很惨，毕竟按顺序叙述，很难把所有线的内容糅合进去，但是那么好的原作做成了这个奇形怪状真的没法说。最精华的冬篇做的实在太赶了，导致最精华的部分和传说的叙事，轮回的结构都没描述清楚。gal改大概除了白2都是挺失败的，结果白2 cc篇也没影子了。想看cc雪菜线动画化。啊好难受啊……为啥游戏改没有大制作相中呢。看到自己很喜欢的游戏改成这个鬼样子还是有点伤心的。 宅腐恋 + 多田君这俩放一块其实也是因为类型问题。冲着动画工坊看了多田君不恋爱，略失望，叙事有点乱，不够轻松，老老实实做月刊少女2不好么？ 宅腐恋还成，日常看着舒服，对于爱看日常系的人来说值得一看。 工作细胞放心去看吧，没啥大问题，也没啥特别好的地方。题材很有趣，我觉得okay。 天狼不知所云，逻辑诡异，评价不高，感觉是给洋鬼子交差的作品。设定大乱炖，打斗倒不错。 上学+碧蓝+游戏三人娘都可以看，还是挺有趣的 我英第三季神野之战吹爆，我觉得比漫画做的好，看的我热血沸腾。这季收尾节奏不好，临时执照片不该那么多原创拖剧情节奏的。但是冲着神野之战我可以给10分，骨头社吹爆！第四季制作放了，稳得一批。 overload看过1 2可以看，说实话我并不喜欢这番，看也都是跳着看。感觉可以不带脑子当无脑看爽文还可以，不过不失的作品吧……感觉价值观有点不对，不是很推荐 喰种想不出谁给它的勇气分割放送，做的跟ppt一样，分镜混乱，剧情跳跃，但是冲着漫画圆满完结的份上给点面子，继续看看吧。监狱篇和流岛篇其实还可以。 p5绝望级别的制作。回头再打一遍游戏解毒吧，动画别看了，灾难级的作画。 附录其他的一时半会想不起来了。回头再说。10月7刀剑来啦，年番，目前微博日推大佬放出的片段可以说制作很稳。有的看了。]]></content>
      <tags>
        <tag>杂谈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[中秋快乐]]></title>
    <url>%2F2018%2F09%2F23%2Fphoto%2F</url>
    <content type="text"><![CDATA[大学以后头一次在家里过中秋，祝大家中秋快乐啊 =-= 千里共婵娟]]></content>
      <tags>
        <tag>杂谈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[训练杂谈]]></title>
    <url>%2F2018%2F09%2F15%2Fmedicine%2F</url>
    <content type="text"><![CDATA[dl杂谈其实今年的文章总体有这样一种趋势，除了一些结构上具有开创意义的创新，大家都是小修小补，想发文章就找个差点的baseline飚一飚，比赛就找最好的部件拼一拼开始加trick开始大炼丹。其实都无外乎网络加深提feature再大batch到小batch交替摩擦。loss不降？查查数据，没问题加几层。上大batch加lr再小batch磨一磨，反正有BN这种东西以后大batch满天飞。所以说啥炼丹心得可说还真不容易总结。在这也就是随便一聊。 不过这倒是可以先谈个问题：你train的真的准么……为啥我问这个问题？你可以自己手写几个层，跟pytorch和tf封装好的层测测输出误差……其实很好奇，为啥pytorch调的cudnn，误差居然比tf大。我有点没搞懂到底谁有问题了。最神奇的还是跟手写的卷积都不一样……如果测试没问题，那还是tensorflow的更准一点，不排除因为浮点精度问题出现了累积误差，但是这确实也挺难搞懂的，回头有时间仔细看看pytorch的卷积怎么写的。 training技巧其实就实验科学的结果来说，这东西真的是个玄学。下面先讲讲我自己的实验习惯，解释的话可能过段时间再详细开一篇聊聊。 首先我们讲一讲选模型的原则。首先如果需要深层次的特征，抛开任务，提特征的时候有人都会偏爱直接选择resnet之类的比较深比较大的网络，但是很多时候在试验的情况下发现loss一直不降，震荡，不收敛，这时候……先看看数据集或是喂的数据有没有问题才是第一要务。先检查自己prepare的数据是否没问题，比如检测自己的crop是不是有问题，或是在其他的一些数据处理函数上可视化view一下。 12如果本身喂进网络的数据没有问题，检查一下数据集。如果样本数量本来就不多，可以考虑先用类似的小一点模型，比如resneXt置换成GoogLenet，设置好超参数。如果在较小的网络上这组超参可以收敛，再把这组参数迁移到大模型上，如果还是不收敛或是还在震荡，可以设optimizer为adam，调小一点batch，然后看看能不能在训练中跳出局部极值。跳出去以后可以再选用大batch size和之前在小模型上使用的超参继续训。 说到这可能有入坑小伙伴还没搞懂大batch到底有个啥用，或者batch size这个参数到底影响了什么……其实引入batch normalization后，确实大batch带来的收益更诱人了，目前大量的网络结构也加了相当多的bn层，这样使用大batch有了很好的条件。在这我先简单解释下batch size的一些作用或者意义： 首先我们谈谈大batch的作用。以sgd举例，事实上sgd大家都知道选一批样本去更新梯度，学这个梯度下降的方向。我们选用大batch，样本更有代表性，因为选择的样本越多，下降肯定越趋于整个数据集收敛应该朝向的方向，所以大batch会更稳定，基本不会在局部震荡， 因为每次参数更新所用到的数据越多，越能代表整体损失函数的梯度，因此梯度精确度更高。另一点是快，而且可以更好利用矩阵计算库里大矩阵乘法的效率，还可以提高显存利用率。 那小batch自然没有上述的好处，取一个极端例子，如果我们把batch size设为1，这样称为在线学习，每次修正方向以各自样本的梯度方向修正，难以达到收敛。可以理解为因为每次梯度更新只用一个样本，所以后果就是很可能在某一个局部极值点附近震荡……同理可辐射到其他小batch size上，但是同样也有好处，可以在达到一定程度后用来精细的去磨一下你的模型。因为小批量在学习过程中加入了噪声，会有一些正则化效果，但是需要小的学习率保持稳定性，否则还是会跳进局部极值难以自拔 对于大batch size，达到相同精度需要更多epoch，因为每轮训练中的迭代次数更少。 所以可以总结下，适当增大batch size好处在于： 内存利用率提高了。 对于相同数据量的处理速度更快。 在一定范围内，一般来说batch size 越大，其确定的下降方向越准，引起训练震荡越小。 但是盲目增大绝对不行，毕竟首先不是所有实验室都有那么好的设备，batch size上去了，首先也得有足够内存和显存支撑。 另外根据上面提到的要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢。主要还有一点，batch size大到一定程度，下降方向已经基本不再变化，加了也没什么用。 所以说本质上还是需要考虑到你的任务到底塞多少合适，另外你的设备到底能怎么撑。一般来说，取2,4,8这么多块卡并行跑，选择一个合适的batch size塞显存就很重要了，因为要考虑显存占用率，你的实验室基友们也不是不跑实验的……不过一般来说，开始训勇敢取个大batch并没啥坏处，有bn撑腰，一切显得美好很多。 另一方面普遍要动的超参就是学习率了。嗯……这没啥好说的，为了训练稳定自然都愿意选小一点的lr或是先大lr降，不收敛不稳定再换小lr跳出震荡。其他自引入超参，emmmm，仁者见仁智者见智，其实是闻者伤心听者流泪……都是调参血泪史。 当然了，上面说的一切都要看你的实验室/公司项目组有没有钱，毕竟钱才是科学第一推动力。感谢以前搭集群的师兄们，真的，实属带善人。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch升级和imread区别]]></title>
    <url>%2F2018%2F09%2F01%2Fiocv%2F</url>
    <content type="text"><![CDATA[前段时间写了一版multiposenet的结构，调试的时候发现了几个问题。 首先是pytorch0.4.0到0.4.1的更新。这个版本的更新里引入了一个比较有用的降采样函数，torch.nn.Functional.interpolate。文档上介绍的也很明确，调用时可以指定升/降采样的size或者一个尺度因子scale_factor。之前其实一直没有一个官方给出的降采样函数或是层，原来融合多个size每次都要复用手写的层，某些时候还不知道对不对的上别人文章的降采样方法……这回添加了一个官方的降采样方法，这样可能细节上可能大家都能统一一点，实验代码也好写一点或是复现一点，而且可以下采样到任意size或是scale，避免了很多方法实现上的微妙区别。load模型的时候跟0.4.0没什么区别，不像0.3.0升0.4.0时由于upsample变动导致我大量模型load不能的囧境。 另外就是关于skimage.io.imread和cv2.imread的一个细节问题，读灰度图时，skimage的读取会读出2通道，而opencv中不添加灰度的flag的时候传入会默认处理成三通道。一开始我随手拿io.imread读了原图，原灰度图是这个样子： 训练跑到一半的时候突然发现tensor运算的时候左右值维度不匹配，才发现了这个细节问题……之前一直都是用cv2.imread,也不会往里传grey的相关参数，直接默认读出三通道，但是灰度图直接拿io.imread调用时会出现shape只有(width,height)两位的问题。去翻了翻skimage的源码，as_grey参数已经deprecated了，其中有这么一段处理：1234567if img.ndim &gt; 2: if img.shape[-1] not in (3, 4) and img.shape[-3] in (3, 4): img = np.swapaxes(img, -1, -3) img = np.swapaxes(img, -2, -3) if as_gray: img = rgb2gray(img) 内部在ndim大于3的情况下，flag设为True也只是会粗暴的直接转化，ndim为2的时候他就直接读出(height,width)维的数组了，所以说这个确实……cv的读取就避免了这个问题。看来还好以前都是直接调cv.imread，这次因为要调transform顺手调了io，以后还是得注意数据喂进去到底是啥……以前也出现过喂错东西跑瞎实验的情况，毕竟数据驱动型的工作，数据的纯净度、形式组织还是至关重要的，得涨点记性。 另外就是multiposenet的子网络问题，目前我也不知道作者在test-dev上的统一size到底用了多大。而且按照原文中的结构，不给出一些细节，最后的ground truth和heatmap的size是绝对不匹配的，目前我强制采样到了256x256，有理解的朋友可以交流一下，代码repo在这：https://github.com/IcewineChen/pytorch-MultiPoseNet]]></content>
      <categories>
        <category>细节</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[新评论gitalk系统踩坑]]></title>
    <url>%2F2018%2F08%2F30%2Fgitment%E2%80%94%E2%80%94config%2F</url>
    <content type="text"><![CDATA[放弃issue形式的评论了。已经在新blog启用valine了，issue有更新会比较烦人。 选择过程现在博客已经改成valine了，对一些不常用github的访客来说，评论更简单，如果想用gitalk，可以参照本文。 网易云跟帖和多说都挂掉了，所以急需一款新的比较好用的跟帖评论工具。比较了下面几种： hexo官方文档里的扩展：disqus，因为墙你懂得 来必力 同样存在加载慢的问题 gitalk 看起来不错，依托github issue, 支持markdown hypercomments，唯一缺点不支持markdown 好吧那就选gitalk吧 实际工作其实按照网上多如牛毛的添加gitalk教程来就行，我随手在这贴一个：https://www.jianshu.com/p/9be29ed2f4b7 ，感谢这位的分享，虽然我大概看的不是他的分享……只是表达一下随手就能搜出一斤的情况。 那说好的坑呢？ 别急……有俩坑。第一个坑在github应用创建的时候，如果你绑定了自己的个性化域名，homepage url和回调url一定填自己绑定后的域名。这个我开始绑定的github.io，回调就报错。 第二个坑就出在网上花花绿绿不同教程上了。主要是gitalk配置字段的问题。不论你选择哪一种方法，都会有在主题配置文件添加字段的选项，如下：123456789# Gitalkgitalk: enable: true clientID: 'your clientID' clientSecret: 'your clientSecret' repo: Blog_comments owner: erbiduo admin: erbiduo distractionFreeMode: true 12345678gitalk: enable: true githubID: github帐号 # 例：asdfv1929 repo: 仓库名称 # 例：asdfv1929.github.io ClientID: Client ID ClientSecret: Client Secret adminUser: github帐号 #指定可初始化评论账户 distractionFreeMode: true 这是我在网上找的两个版本的主题文件配置内容，你猜哪个是对的？ 这就需要看你在themes/your_theme/layout/_third-party/comments/gitalk.swig内的字段怎么写的了，swig文件添加进去的内容：12345678910111213141516&#123;% if page.comments &amp;&amp; theme.gitalk.enable %&#125; &lt;link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css"&gt; &lt;script src="https://unpkg.com/gitalk/dist/gitalk.min.js"&gt;&lt;/script&gt; &lt;script type="text/javascript"&gt; var gitalk = new Gitalk(&#123; clientID: '&#123;&#123; theme.gitalk.ClientID &#125;&#125;', clientSecret: '&#123;&#123; theme.gitalk.ClientSecret &#125;&#125;', repo: '&#123;&#123; theme.gitalk.repo &#125;&#125;', owner: '&#123;&#123; theme.gitalk.githubID &#125;&#125;', admin: ['&#123;&#123; theme.gitalk.adminUser &#125;&#125;'], id: location.pathname, distractionFreeMode: '&#123;&#123; theme.gitalk.distractionFreeMode &#125;&#125;' &#125;) gitalk.render('gitalk-container') &lt;/script&gt;&#123;% endif %&#125; 很明显，如果在部署的时候解析你主题文件的配置，模板字段里Gitalk对象肯定要和配置文件里字段一致，也就是说你的配置文件里gitalk下其他字段要跟Gitalk对象内的字段一致，gitalk.ClientID对应在主题配置文件下的就是ClientID字段，owner:gitalk.githubID和主题配置下githubID字段要这样一一对应，我按照某些教程搭建的时候就出了问题，调试了一下找到了这俩坑。其他的基本就按教程配置就行，最后注意swig中字段和theme下的config.yml一致就可以。]]></content>
      <categories>
        <category>技术杂谈</category>
      </categories>
      <tags>
        <tag>杂谈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[教师论]]></title>
    <url>%2F2018%2F08%2F30%2F%E6%95%99%E5%B8%88%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[缺德请挨打。 https://www.mysupervisor.org/viewtopic.php?f=1096&amp;t=50671&amp;sid=18f3512f1b806b030a741a6746af0969]]></content>
      <tags>
        <tag>杂谈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pose estimation tutorials]]></title>
    <url>%2F2018%2F08%2F19%2F%E5%88%86%E4%BA%AB%2F</url>
    <content type="text"><![CDATA[分享一版整理的一些姿态估计心得的repo：https://github.com/IcewineChen/Pose-estimation_tutorials]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[谈谈gluon和其他我用过的框架]]></title>
    <url>%2F2018%2F07%2F29%2Fgluon%2F</url>
    <content type="text"><![CDATA[关于gluon前几天跟小伙伴们讨论到了几个问题，突然讨论这个事情：比如说我的数据load进来之后，想把不同的batch并行，这样该怎么弄？ 我很喜欢pytorch，但是我大部分时间对pytorch的并行调用dataparallel时的实际操作是把数据一个batch拆分，然后并行到设备上，很明显不符合需求，要是改动还得对底层动刀。然后突然想到嗯，好像除了这个问题，有些时候去符号式这种东西，还真不太适合快速复现或者验证想法。目前大部分时间复现我确实都优先考虑mxnet做。如果不太需要自定义层，基本才会考虑pytorch。然后呢……就是安利一发mxnet了。顺带也相当于总结下我对这些框架的使用体验。最早在16年的时候，我猜大家肯定都在耍……caffe。那时候出于我写c++写习惯了的，一度出现了c++是世界上最好的语言的错觉……事实上caffe确实定制度好，自由度高，python接口也问题不大，但是确实需要自定义的太多，编译各种问题吐血，而且出于语言习惯，我更乐意从头写个layer，backward慢慢磨，然而这确实相当浪费时间。而且到后来越来越多的文章都在用tensorflow，出于代码的编写和调试时间成本，还是基本放弃了caffe，除非有时候看一些早期的文章，放出的代码基于caffe还会看一看，其他时候基本不考虑caffe了。caffe2出了以后，detectron似乎也没能完全拯救他，我是编译了一份扔一边了。不过当年在公司实习的时候，大家确实也都在用caffe，我的结论就是是出于部署和性能考量，caffe比较适合。然而mxnet在分布式和部署上我觉得不比caffe差，事实也确实是后续业务也开始转向mxnet。总体而言caffe在科研上开始没用tensorflow或是pytorch那么闪光了。 之后tensorflow。没什么可多说的，基本上你肯定得会。放tensorflow代码的文章多如牛毛，文章多，社区好，早期时候带自动微分这种牛逼功能，背景牛逼，而且符号式编程其实更符合我的思维方式（也可能有人对符号式编程写model不习惯，这个只是个人意见）。然后绝大部分时间我research绝不会写tensorflow的代码。为啥……那个调试真的痛苦的要死，我对tensorflow的更新最终停在了estimator出来的那个版本。虽然后期的eager Execution机制引入动态图，但是本质上依然只是一个功能，整个框架依然是基于静态图开发的。tensorflow对于实现你表达的思路是个非常非常优秀的框架，然而确实会给你中间的一些验证带来很多不必要的麻烦，而且相对比较笨重。对于科研实验来说，很多东西是相当冗余的，对于要求快速实验验证想法的research来说这个问题还是比较致命的。 接下来才是比较重要的比较。我觉得对于大部分来说大概还是偏好pytorch和mxnet一点（其实并没mxnet什么毛关系）。支持动态图，当时刚转到pytorch上的我觉得这简直是最棒的机制了。动态图在调试上优势太大，中间变量查看十分十分方便，对于快速验证模型的一些问题提，比如说中间shape，dtype以及各种问题供了巨大便利，不需要静态图的各种包装以及session去跑。在同样采用动态图的基础上，mxnet和pytorch的核心区别，我觉得大概就在符号编程上了。pytorch继承了torch当年的优良意志：我们绝对不用符号式编程……符号式编程简单来说就是把变量就定义成一个符号，之后把数据扔进来的时候再通过name告诉他哪喂什么就好，相对来说很适合直接扔一个symbol在这，最后再考虑数据的问题就好，可以把模型定义单独作为一件事情。而pytorch数据流向直接定义调用就可以，最后需要求导记得用variable封装好即可。 然而gluon出来以后，我觉得mxnet确实兼顾了各家之长。gluon里两点让我觉得特别优秀。第一点就是不得不提的混合编程，这点看下面代码就知道：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253class Hourglass(gluon.nn.HybridBlock): def __init__(self,n,nModules,nFeats): super(Hourglass,self).__init__() self.n = n self.nModules = nModules self.nFeats = nFeats self.up1_ = gluon.nn.HybridSequential() self.low1_ = gluon.nn.HybridSequential() self.low2_ = gluon.nn.HybridSequential() self.low3_ = gluon.nn.HybridSequential() for i in range(self.nModules): self.up1_.add(Residual(in_channels=self.nFeats,out_channels=self.nFeats)) self.low1 = gluon.nn.MaxPool2D(pool_size=(2,2),strides=(2,2)) for i in range(self.nModules): self.low1_.add(Residual(in_channels=self.nFeats,out_channels=self.nFeats)) if self.n &gt; 1: self.low2 = Hourglass(n=n-1,nModules=self.nModules,nFeats = self.nFeats) else: for i in range(self.nModules): self.low2_.add(Residual(in_channels=nFeats,out_channels=nFeats)) for i in range(self.nModules): self.low3_.add(Residual(in_channels=self.nFeats,out_channels=nFeats)) self.up2 = gluon.nn.Conv2DTranspose(channels=self.nFeats,kernel_size=(2,2),strides=(2,2)) def hybrid_forward(self,F,x): up1 = x for i in range(self.nModules): up1 = self.up1_[i](up1) low1 = self.low1(x) for i in range(self.nModules): low1 = self.low1_[i](low1) if self.n &gt; 1: low2 = self.low2(low1) else: low2 = low1 for i in range(self.nModules): low2 = self.low2_[i](low2) low3 = low2 for i in range(self.nModules): low3 = self.low3_[i](low3) # gluon的nn里没有upsample，混合直接调用ndarray的sample # up2 = F.UpSampling(low3,sample_type='bilinear',scale=2) up2 = self.up2(low3) return up1 + up2 这是我定义的一个Hourglass结构，只需要你的net或者模块继承hybridblock，forward时候就需要实现一个hybrid_forward(self,F,x)。这个F是区别于非混合式forward的量，调用时候F.function就会自动找你的function从什么里调用，是从符号式还是正常数值计算的ndarray的包里去找。这个功能真的相当人性，一部分ndarray中实现的函数，在symbol包内或是gluon.nn包内是没有的，这样使得mxnet的可用性和使用体验更好，用户大概只需要关注结构定义就可以。当然逻辑比较简单的网络可以直接Sequential add即可。 二是几乎没什么学习成本。这个结构看着熟不熟悉？基本就和pytorch里的结构定义是一样的。有大量的人都在主用pytorch，这样的风格从pytorch迁移代码十分方便。比起mxnet symbol相对更繁琐的定义，这种class内部定义层，再在forward连接的方式大家确实写起来十分习惯，而且也比较符合我脑子里想像的dl定义范式和思维习惯。mxnet目前唯一的问题大概api更新太快，之前每一版更新变动都不小，还处在高速发展，其实也就是不太稳定的阶段，但是bug也已经减少了很多，可用性高而且复现好用，个人觉得框架选择的话，pytorch最轻松上手，mxnet大概是使用起来感觉最好的。如果硬要选框架自己写代码，主役pytorch或是mxnet都是好选择，然而考虑到科研中的泛用度，mxnet确实略逊pytorch一筹，mxnet代码的文章不是很多。但是就个人喜好而言，mxnet在gluon引入以后可能更好一点。]]></content>
      <categories>
        <category>mxnet</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>mxnet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实验代码分析]]></title>
    <url>%2F2018%2F07%2F16%2Fhg-3d%2F</url>
    <content type="text"><![CDATA[上次组会分享了CVPR2018的3d pose估计文章 3D Human Pose Estimation in the Wild by Adversarial Learning,文章中提到一种用GAN的方式生成3D in the wild估计的方法。最近准备复现，顺带在这里先分析一下结构中Generator，顺带分析一下代码 生成器的结构参见Towards 3D Human Pose Estimation in the Wild: a Weakly-supervised Approach，感谢作者，该结构以stacked hourglass为base network做一个2d keypoint估计，然后扔进3d regression得到最后的结果。数据集选用了有3d ground truth和没有ground truth的in the wild 2d集拼合，整体结构上在2d估计中抽出了中间层信息送入回归器，迁移了in the wild的信息。 pytorch网络代码分析先贴一段核心网络结构的代码：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980class HourglassNet3D(nn.Module): def __init__(self, nStack, nModules, nFeats, nRegModules): super(HourglassNet3D, self).__init__() self.nStack = nStack self.nModules = nModules self.nFeats = nFeats self.nRegModules = nRegModules self.conv1_ = nn.Conv2d(3, 64, bias = True, kernel_size = 7, stride = 2, padding = 3) self.bn1 = nn.BatchNorm2d(64) self.relu = nn.ReLU(inplace = True) self.r1 = Residual(64, int(128)) self.maxpool = nn.MaxPool2d(kernel_size = 2, stride = 2) self.r4 = Residual(128, 128) self.r5 = Residual(128, self.nFeats) _hourglass, _Residual, _lin_, _tmpOut, _ll_, _tmpOut_, _reg_ = [], [], [], [], [], [], [] for i in range(self.nStack): # n阶的hourglass _hourglass.append(Hourglass(4, self.nModules, self.nFeats)) for j in range(self.nModules): # 连接stacks间的基本残差单元 _Residual.append(Residual(self.nFeats, self.nFeats)) lin = nn.Sequential(nn.Conv2d(self.nFeats, self.nFeats, bias = True, kernel_size = 1, stride = 1), nn.BatchNorm2d(self.nFeats), self.relu) _lin_.append(lin) _tmpOut.append(nn.Conv2d(self.nFeats, ref.nJoints, bias = True, kernel_size = 1, stride = 1)) _ll_.append(nn.Conv2d(self.nFeats, self.nFeats, bias = True, kernel_size = 1, stride = 1)) _tmpOut_.append(nn.Conv2d(ref.nJoints, self.nFeats, bias = True, kernel_size = 1, stride = 1)) for i in range(4): for j in range(self.nRegModules): _reg_.append(Residual(self.nFeats, self.nFeats)) # append部分对应stackhourglass中的沙漏型结构，将其中用的residual模块和n个Stack整理进sequential的形式方便之后再循环里调用 # modulelist的取法其实基本跟list一样，封装在modulelist里会检测到每一个的hook，直接包装在list里，每一个单元内部的参数会初始不到 self.hourglass = nn.ModuleList(_hourglass) self.Residual = nn.ModuleList(_Residual) self.lin_ = nn.ModuleList(_lin_) self.tmpOut = nn.ModuleList(_tmpOut) self.ll_ = nn.ModuleList(_ll_) self.tmpOut_ = nn.ModuleList(_tmpOut_) self.reg_ = nn.ModuleList(_reg_) self.reg = nn.Linear(4 * 4 * self.nFeats, ref.nJoints) def forward(self, x): # 将定义了的层按顺序传递，构建整个网络的forward顺序 x = self.conv1_(x) x = self.bn1(x) x = self.relu(x) x = self.r1(x) x = self.maxpool(x) x = self.r4(x) x = self.r5(x) out = [] for i in range(self.nStack): hg = self.hourglass[i](x) ll = hg for j in range(self.nModules): ll = self.Residual[i * self.nModules + j](ll) ll = self.lin_[i](ll) tmpOut = self.tmpOut[i](ll) out.append(tmpOut) ll_ = self.ll_[i](ll) tmpOut_ = self.tmpOut_[i](tmpOut) # 取多层信息，作为下一个hourglass的输入，也保存信息到送入reg的x里 x = x + ll_ + tmpOut_ for i in range(4): for j in range(self.nRegModules): x = self.reg_[i * self.nRegModules + j](x) x = self.maxpool(x) x = x.view(x.size(0), -1) reg = self.reg(x) out.append(reg) return out 这就是3d stacked hourglass结构的代码。关于一个nn.Module的代码怎么写，其实参照pytorch源码的Module写就可以，由于自动微分这种东西的存在，继承Module时需要只定义init和forward函数即可。在init函数中，我们定义需要的层，需要的结构，然后在forward中，定义层的连接，也就是网络的结构。这段代码描述了生成器的结构，总体来说，nStack就是单个stacked hourglass的阶数，modules是使用级联的模块数，residual是一个残差结构。ll是我们一开始抽出来的stacked的输出层和中间层的和，也就是抽出层再加上输出的结果，之后在输入维度上做整合。reg是3d信息回归器的结构。在forward中，按照stacked hourglass的方法先加了conv+bn+relu+残差+max pooling的结构调整网络输入，之后搭建2d的级联结构，同时把每个stacked结构中的中间一层抽出来,和每个级联的stack一起送去3d regression。具体的代码注释和一些修改我都放在了我fork的repository里，欢迎来看整个注释和代码啊，现在还没彻底完成，还在这个的基础上加入GAN部分的代码（主要是multi-source的鉴别器）去复现GAN-hg-3d。准备在这个结构上调整，看看in the wild的3d pose的效果如何。大概觉得这个方向在工业界落地上可能还有点价值……以前实习的时候组里也有人做过2d的gesture，问题本质上都差不多。理论上行为这一块市场很大，pose的工作还是很有价值的。 pytorch在大部分时间是不需要定义backward的，基本上直接继承nn.Module就可以实现。不过有时候需要我们自定义loss，这时候loss从function里继承，这里其实本质上你也可以直接写在新的Module里，但是你可以尝试一下在Module的backward里面加上一些print，之后从外部调一下试试看。这时候是不会执行的。因为本身Module底层就是要进行backward的操作，每一步操作的时候都会生成一个Function的子类，最后把这些子类的forward和backward连接到一起。所以这些时候你在module内部写一个backward它根本就不会执行，因为实际上真的调用的是基于Function的backward操作。所以如果有大量自定义而pytorch的底层又没有实现的计算，可以直接自定义一个继承自Function的class，然后再用module封装进去。这也是官方文档的做法，在neural style transfer的那篇tutorial里很好的表示了loss的写法，想再自己动手试试看看的话可以去看一这篇文档。根据我的经验你大概只需要记住：绝大部分时候你不需要自己写backward，但是比如你要实现一种新的下降算法或是已经提供的计算不能满足的层定义，那再自己去写。 那么如果自定义一些操作，请遵照一下结构：123456789class CustomLoss(torch.autograd.Function): def __init__(self,*kwargs): super(CustomLoss,self).__init__() def forward(self,*kwargs): raise NotImplementedError def backward(self,) raise NotImplementedError 按照function继承是要定义backward的，对grad要进行底层实现，基本上只有loss这部分的定义需要费点事。mxnet的定义倒是可以根据符号编程自动推导，符号式编程加自动微分确实很方便，不过sym的调试其实并没有pytorch的动态图来的直观。感觉mxnet在复现上还是更简单快捷粗暴，不过文档不全和有些地方因为更新太快有bug，稍微有点脱了后腿。pytorch这次的更新据说填了不少坑,就我的感受来说，内存不那么容易被多输入Variable的索引跑到炸了，之前的话以Variable对接dataloader在梯度更新后经常要注意取loss[0]，否则直接会加入Variable中的各种参数。据说目前版本解决了这个问题。 数据集接口没什么太多问题的，dataloader只需要注意cv读和dataloader要求的维度顺序，还有定义最后混合的fusion结构中要构建包含2d ground truth和3d ground truth的样本即可。getitem基本不需要做多少改动，只需要按顺序，3d+2d顺序append，之后索引在3d样本数内读3d，大于部分get 2d样本即可。 关于版本更新导致的问题其实原repo的作者是给出了pretrain的模型的，但是在Stage2阶段是用不了的……我单点打到这一行，发现load的模型是有问题的，这一行定在upsample层上： self.up2 = nn.Upsample(scale_factor = 2) 事实上upsample需要一个align_corners的参数，在0.4.0之前这个参数应该被直接给了None，导致在0.4.0版本load pretrain的模型后，跑到upsample层的forward时在module的检查时一直报错，会一直报给你检查不到align_corners。再打进内部检验的断点时，发现确实load的模型align_corner是None。如果使用我修改后的pytorch0.4.0兼容的代码train的模型是不存在这个问题的，然而我检查的时候依然没有找到align_corner参数，理论上应该default是False。这可能是底层实现有改动，更底层的部分我就step out没管了。这个大概是版本更新导致的bug，具体有待继续调试验证。如果想看pytorch修改后能用的代码，可以直接跑我fork后的fixed branch，里面已经做了对应的各种修改，在python2.7+ubuntu16.04+pytorch0.4.0下可以正常运行。 后续工作之后会打算对文中内容复现。不过目前遇到一个问题是关节点depth map怎么得到。这个问题目前没找到什么合适的代码或是工具，如果有大佬知道的话希望能邮件或是github issue给我……非常感谢]]></content>
      <categories>
        <category>deep learning</category>
        <category>paper reading</category>
        <category>pose estimation</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>pytorch</tag>
        <tag>human pose estimation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[没有任何意义的高亮测试]]></title>
    <url>%2F2018%2F07%2F13%2F%E9%AB%98%E4%BA%AE%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[123456#include &lt;iostream&gt;using namespace std;int main(void)&#123; cout &lt;&lt; "highlight" &lt;&lt; endl;&#125; 意义不明的测试之前想写多写点代码分析和最近跑的实验代码的剖析，结果发现代码高亮不了……现在hexo的代码高亮卡的比较严，首先得先把site下的config里的highlight字段的几项变为true（之前的auto_detect是false），再把主题下的config里的highlight的主题修改成自己想修改的即可，hexo里自带了5种高亮。 下一篇填坑=-=之后周末填个坑吧……pytorch0.4其实最近跑了一下好多坑都被填完了。周末随便写点mxnet和pytorch的代码分析和总结吧。就酱。想回家睡两觉。不过想先把最近看的番总结一下，顺带给island安利一波……非常超神的游戏，可能这两天给今年一月番做个总结，讲讲游戏，开个游戏块吧……这个博客大概以后会频繁更更，也当发发牢骚了，顺带在漫评路上走一走……多说和网易云跟帖都挂了，所以评论暂时就不开了]]></content>
      <categories>
        <category>杂谈</category>
      </categories>
      <tags>
        <tag>杂谈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人体姿态估计方法综述]]></title>
    <url>%2F2018%2F07%2F08%2Fhuman-pose-estimation%2F</url>
    <content type="text"><![CDATA[从上学期末到现在，在姿态估计方面的工程和论文、实验也算做了不少。现在想着总结一下，理清一下思路，对之后的研究和实验也能提供一点帮助。pose estimation是个很有意思的课题，目前2d的pose已经做得很成熟了，in the wild或是多人任务都已经非常不错了，3d pose目前还有很多潜力可挖。2d pose多用heatmap特征，ground truth多以keypoints的ground truth坐标高斯分布内生成。3d上方法普遍还是以2d的结果+深度信息的回归做框架。不过最近读了一篇CVPR2018的3d pose工作，觉得3d pose在in the wild上还有很大潜力可挖，在这里也会分享一下。 研究概述2d pose estimation从CVPR2016到AAAI2018，2D &amp; 3D human pose estimation（姿态估计）浮现了不少好的方法。按时间顺序来说，我大概印象比较深刻的有： Convolution pose machine，出自2016的CVPR。论文地址(https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Wei_Convolutional_Pose_Machines_CVPR_2016_paper.pdf) ，CMU的工作，一种2D pose估计的方法，利用多个不同scale的effective Receptive Field对应的heatmap进行姿态估计。我个人觉得这篇工作其实很不错，之前做的项目用到过cpm的model做预测，对于固定摄像角度的单人姿态估计非常不错，不过model嵌入到系统里速度和效果不如下面提到的openpose，而且工作在2018年视角看来也比较老了……不过我觉得CPM工作挺经典的，我最早接触的工作也是这篇。 Stacked Hourglass Network，论文地址（https://arxiv.org/abs/1603.06937) ，非常经典的工作，很多后续的工作都以stacked hourglass作为backbone或者baseline，许多3d pose的工作也借鉴了这一结构。这篇文章提出的stacked hourglass的方法，用了一种downsample-upsample的encode-decode的结构。stacked hourglass使用了一种residual module的初级模块，采用了卷积路加跳连路，跳连路卷积核保证单元输入输出channel相同。之后以residual module为base结构，逐级降采样后再重新升采样，每一个降采样路会跳出一个跳连路以保留原尺度，再与升采样后同一尺度的输出相加，整个结构图如下所示 鼎鼎大名的openpose库的文章，出自2017的CVPR。Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields，也是来自CMU的工作。这个work工程价值也很高，在github上也开源了openpose库，自带demo里有一个多人场景下的pose estimation demo。这篇文章和工作主要贡献在提供了一种多人场景下的姿态估计方法，不同于许多先做检测再做pose的自上而下方法，这篇文章直接采用了自下而上的思路，将关键点heatmap和人的关节之间的连接策略一起得到。base network采用了一种multi-branch的结构，其中一路用于提keypoint的confidence map，另一个branch用于提PAF，也就是用于关节连接的特征，每一路都有多个stage，最后自然而然地实现了多人场景下的pose estimation。这篇文章的网络结构如下所示 pyramid residual network，出自ICCV2017的文章。 文章全名learning feature pyramids for human pose estimation。可以直接去ICCV2017的库里搜。这篇文章好在写的非常详细……怎么说呢，就是读起来非常非常舒服，文章结构很好。其实核心思想就是在stacked hourglass based network的基础上，将residual module替换成了pyramid module，pyramid residual module利用了输入的多个scale，在性能上得到了一定的提升。文章还提到了这种multi-scale的网络初始化的一些trick，并且设计了一个实验来验证residual network中将shortcut一路改成BN+relu+3x3卷积效果更好（虽然我实在想不明白为什么）……不过我觉得这文章的写法和思路非常清晰，所以拿出来想说一下。下面给出prm单元的结构，就是这种利用了输入的多个scale的multi结构，也算是比较巧妙 3d pose estimation接下来想提两篇3d pose estimation的工作。也可以说是两篇半……因为有一篇其实是听组内分享时候学习的=-= 首先简单提下这半篇吧。出自A simple yet effective baseline for 3d human pose estimation。以2d的骨架作为输入，然后回归直接得3d骨架信息。整个结构如下所示。延续了2d+regression的思路。 这周末读了两篇in the wild的3d pose工作。第一篇是Towards 3D Human Pose Estimation in theWild: aWeakly-supervised Approach， ICCV2017的工作。这篇文章利用了一种弱监督的思路，将2d和3d dataset放在一起训练，利用一些先验经验定义了一个2d数据集生成结果的3d 回归loss。该方法的数据集combine了2d和3d集，3d集有3d pose的ground truth，但是3d数据集都是出自lab环境，起不到in the wild的无约束效果。那怎么办？引入2d集做弱监督。整体网络结构还是先用stacked hourglass做2d预测，不同的是这次利用了stacked hourglass中多个层的信息作为3d回归模块的输入，再去进行3d回归，这样在3d回归的模型中，引入的in the wild的2d数据集的弱监督就会将constraint引入了结果，实现了in the wild上的性能提升，我在这理解为一种transfer的trick。这个思路也在后面的基于GAN的工作中作为generator。整体结构如下 之后是我这周末读的另一篇工作，跟PRM出自同一个组的工作，3D Human Pose Estimation in the Wild by Adversarial Learning，出自CVPR2018。整体思路是用GAN改善一下上面这篇的work。generator直接选用了上一篇的结构，在discriminator上，用了一种多输入的结构： 在其中，geometric descriptor是将ground truth和sample的x,y,z坐标之差和平方直接取pairwise形式的6D向量，2d heatmap和depth map直接就是从2d和3d module得到，这个结构值得借鉴。discriminator将所有3d信息编码后以1为y算一个binary entropy loss，2d sample以0为y去算loss。训练时，先train生成器，再轮流train discriminator和generator即可。 总结到目前来说，pose estimation是一个还算比较热门的课题。目前看来工作相关热度不低，而且还有些能做的地方，比如说3d pose目前的工作就不是很多，2d多人估计的方法还需要进一步改善，目前最好的是sjtu提出的alphapose，不过该库沿用了自顶而下的设计，所以我觉得基于PAF这种连接特征的自底而上的方法还有探究空间。之后不太忙的时候可能把network代码拉出来分析分析再写一篇吧……]]></content>
      <categories>
        <category>deep learning</category>
        <category>paper reading</category>
        <category>pose estimation</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>pose estimation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[batch hard triplet loss]]></title>
    <url>%2F2018%2F07%2F05%2Fbatch-hard%2F</url>
    <content type="text"><![CDATA[I have reproduced a mxnet-version of batch_hard triplet loss.If you want to get the code, click this website(https://github.com/IcewineChen/mxnet-batch_hard_triplet_loss). I achieve the batch hard in this repository. BasedBased on (https://arxiv.org/abs/1703.07737).If you want more details of batch hard triplet loss, read this paper please. It’s really a good work. Introduction In this paper,authors propose a new format of triplet loss called batch hard Efficient for re-id tasks Inplement the hard mining method and soft-margin Can be used in re-id tasks Architecture Used resnetV2 to get 128-dimensions embeddings Used triplet loss to train embeddings 大概意思其实就是我用mxnet复现了一版batch hard triplet loss的代码。最早我是在re-id task相关的文章上看到了这篇方法，原文在arxiv上，17年出的……当时用了作者给的repo跑，实验了batch hard和batch all两种形式的triplet loss，发现效果相当不错，思路写得也相当清晰（具体文章请见上面给的链接哈=-=），不过paper reading太久之前做的了，可能之后再用到这种思路的时候会写一篇paper reading整理思路。不过其实为什么work，文章的思路已经写得很明白了，hard mining。batch-all的方法利用了全部可以使用的三元组，将样本中的三元组利用大幅提高，这种trick非常简单粗暴，但是确实在batch-hard前在market1501的benchmark上非常行之有效。但是batch hard的方法从另一个角度入手，对batch内的样本进行hard-mining，去学习极端的三元组。这个hard-mining的过程简单可以概括为选最难的，也就是对选取的anchor距离最远的正样本，同理再选最难的负样本组成一个三元组，这样就选择了最有效的三元组，收敛后效果一定是比原始的triplet loss效果要好的。这个思路非常的简单易懂，但是确实很work。为了快和方便调试我就做了一版mxnet版本的batch hard的代码。tensorflow版本调试的时候确实比较尴尬……相比之下同样采用符号式编程思想的mxnet调试相当简洁，动态图中间的值可以各种print和查看，而且dataloader也方便调试，继承以后主要就注意两点：一是shape二是变量名，内部的检查机制主要就这两件事，由于值在动态图计算过程中打断点到内部很简单，所以调试起来也非常爽快。如果对这版代码或是相关工作感兴趣，请戳我的github主页，链接戳这里：(https://github.com/IcewineChen/mxnet-batch_hard_triplet_loss)]]></content>
      <categories>
        <category>mxnet</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>mxnet</tag>
        <tag>triplet loss</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[caffe安装]]></title>
    <url>%2F2017%2F09%2F04%2Fcaffe-installation%2F</url>
    <content type="text"><![CDATA[关于安装本篇讲述的是关于A卡rmbp的caffe安装与编译中一些问题的解决。 Caffe版本我download下的是目前最新的caffe1.0。关于download caffe1.0的包，直接git clone下来 git clone https://github.com/BVLC/caffe.git 配合食用的工具brew基本都用。没有下载homebrew的盆友请如下操作安装homebrew及依赖项12/usr/bin/ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"brew install wget 之后的caffe编译过程建议选择cmake，cmake使用在后面会说。我们会选用brew进行安装 需要的环境1.python与c++环境准备首先是关于python的问题。之前本地上安装的2.7和3.6，但是在安装过程中改动Makefile的过程中始终会导致numpy连接到System下的pythonframework里。由于这个问题有两个解决方法，后面会详细说道。如果不想释放权限，可以选择安装anaconda，对于之后Makefile.config中的配置比较友好。如果想关掉Rootless对Python.framework中的文件进行编辑，选择进入恢复模式csrutil disabled即可。然而这种头很铁的行为我不推荐。之后会以anaconda配置python的方式便于进行caffe python接口的编译。BAIR的install文档关于libstdc++ installation的部分没什么需要多说的，基本上都不会因为标准库困扰。 2.关于anaconda不能翻的盆友可以直接去tsinghua找一份anaconda的安装包，为了更好的支持caffe1.0，建议选择python2.7版本进行下载。这给一份下载链接： https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda2-5.0.0-MacOSX-x86_64.pkg 之后懒人包安装。对环境变量进行配置。视个人使用在~/.bashrc或~/.zshrc(宇宙第一fish fish~/.config/fish/config.fish去set)中添加 export PATH=~/anaconda/bin:$PATH 之后关于anaconda的使用问题直接直接-h即可，我们直接使用anaconda建立并激活python2.7环境123conda create --name python27 python=2.7activate python27source activate python27 其他包的管理可以选择pip或者conda install进行。 3.其他依赖项安装根据BAIR的官方文档进行安装。1234brew install -vd snappy leveldb gflags glog szip lmdb# need the homebrew science source for OpenCV and hdf5brew tap homebrew/sciencebrew install hdf5 opencv 如果之前安装过snappy等库，卸了更新，写个脚本sh一下，依赖项也这部分可以参见官方文档1234567for x in snappy leveldb gflags glog szip hdf5 lmdb homebrew/science/opencv;do brew uninstall $x; brew install --fresh -vd $x;donebrew install --build-from-source --with-python -vd protobufbrew install --build-from-source -vd boost boost-python caffe编译安装caffe down下来以后，开始caffe的安装和编译。进入的caffe根目录，以我本地为例如下： 之后准备进行caffe的编译。在这里先要根据你的需要动一下Makefile.config。为了开启python接口和选择只用CPU模式，我们需要对原生makefile的一些部分进行修改首先是CPU_ONLY。将line8的cpu_only=1 uncomment掉，改成只用CPU的模式 之后要进行python和matlab接口安装的配置。如果想要开启matlab接口，将下图的matlab部分uncomment掉并根据你的matlab路径进行设置。我开了python接口的配置。python接口给了你两个选择。没有安装anaconda的话你可以选择将你本地的python2.7路径和numpy/core作为python_include的值，但是每次这样做最后都会发现无法import numpy.core.multiarray，print出numpy路径后会发现这里没有链到lib/python而是链到了/System/Library/Framework中的Python.Frameworks中的numpy库，其中numpy的版本不符合caffe1.0的要求。因此要不改掉rootless，用更新的numpy替代。这里有链接关于此问题的解决： https://stackoverflow.com/questions/20518632/importerror-numpy-core-multiarray-failed-to-import 如果不想这么改动，请启用anaconda进行python环境设置。之前有讲到anaconda的环境设置，之后需要在anaconda设置的环境中装好需要的库即可。此时python路径如下图中的anaconda几行配置完成 原生的caffe本身不带build和cmake这些目录。我们进入到caffe根目录后，先建立一个build文件夹并在其下进行编译12mkdir buildcmake .. cmake过后会贴出一些信息，展示caffe的安装依赖项是否已经安装齐和版本是否满足要求，以及你需要安装的接口、以及模式的设置，请仔细阅读，如有缺失项请回到caffe根目录下make clean清除并重新按照上面的一些步骤与说明安装依赖项，并重复上述make过程。 之后需要对两个文件改动,一个是cmakecache.txt,另一个是caffeconfig.cmake。其中关于CPU_ONLY的设置还是off，所以需要改动。 看完配置信息，如果觉得没问题，继续make make all make all之后会提示你pycaffe的信息，之后 make pycaffe okay，整个过程配置完毕，最后结果如下： 之后去examples跑一下caffe的例子就可以了。可以看一下转化lmdb等等数据格式的一些预备知识在进行食用。]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>caffe</tag>
        <tag>MacosX</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
</search>
