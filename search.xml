<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[中秋快乐]]></title>
    <url>%2F2018%2F09%2F23%2Fphoto%2F</url>
    <content type="text"><![CDATA[大学以后头一次在家里过中秋，祝大家中秋快乐啊 =-= 千里共婵娟]]></content>
      <tags>
        <tag>杂谈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[训练杂谈]]></title>
    <url>%2F2018%2F09%2F15%2Fmedicine%2F</url>
    <content type="text"><![CDATA[dl杂谈其实今年的文章总体有这样一种趋势，除了一些结构上具有开创意义的创新，大家都是小修小补，想发文章就找个差点的baseline飚一飚，比赛就找最好的部件拼一拼开始加trick开始大炼丹。其实都无外乎网络加深提feature再大batch到小batch交替摩擦。loss不降？查查数据，没问题加几层。上大batch加lr再小batch磨一磨，反正有BN这种东西以后大batch满天飞。所以说啥炼丹心得可说还真不容易总结。在这也就是随便一聊。 不过这倒是可以先谈个问题：你train的真的准么……为啥我问这个问题？你可以自己手写几个层，跟pytorch和tf封装好的层测测输出误差……其实很好奇，为啥pytorch调的cudnn，误差居然比tf大。我有点没搞懂到底谁有问题了。最神奇的还是跟手写的卷积都不一样……如果测试没问题，那还是tensorflow的更准一点，不排除因为浮点精度问题出现了累积误差，但是这确实也挺难搞懂的，回头有时间仔细看看pytorch的卷积怎么写的。 training技巧其实就实验科学的结果来说，这东西真的是个玄学。下面先讲讲我自己的实验习惯，解释的话可能过段时间再详细开一篇聊聊。 首先我们讲一讲选模型的原则。首先如果需要深层次的特征，抛开任务，提特征的时候有人都会偏爱直接选择resnet之类的比较深比较大的网络，但是很多时候在试验的情况下发现loss一直不降，震荡，不收敛，这时候……先看看数据集或是喂的数据有没有问题才是第一要务。先检查自己prepare的数据是否没问题，比如检测自己的crop是不是有问题，或是在其他的一些数据处理函数上可视化view一下。 12如果本身喂进网络的数据没有问题，检查一下数据集。如果样本数量本来就不多，可以考虑先用类似的小一点模型，比如resneXt置换成GoogLenet，设置好超参数。如果在较小的网络上这组超参可以收敛，再把这组参数迁移到大模型上，如果还是不收敛或是还在震荡，可以设optimizer为adam，调小一点batch，然后看看能不能在训练中跳出局部极值。跳出去以后可以再选用大batch size和之前在小模型上使用的超参继续训。 说到这可能有入坑小伙伴还没搞懂大batch到底有个啥用，或者batch size这个参数到底影响了什么……其实引入batch normalization后，确实大batch带来的收益更诱人了，目前大量的网络结构也加了相当多的bn层，这样使用大batch有了很好的条件。在这我先简单解释下batch size的一些作用或者意义： 首先我们谈谈大batch的作用。以sgd举例，事实上sgd大家都知道选一批样本去更新梯度，学这个梯度下降的方向。我们选用大batch，样本更有代表性，因为选择的样本越多，下降肯定越趋于整个数据集收敛应该朝向的方向，所以大batch会更稳定，基本不会在局部震荡， 因为每次参数更新所用到的数据越多，越能代表整体损失函数的梯度，因此梯度精确度更高。另一点是快，而且可以更好利用矩阵计算库里大矩阵乘法的效率，还可以提高显存利用率。 那小batch自然没有上述的好处，取一个极端例子，如果我们把batch size设为1，这样称为在线学习，每次修正方向以各自样本的梯度方向修正，难以达到收敛。可以理解为因为每次梯度更新只用一个样本，所以后果就是很可能在某一个局部极值点附近震荡……同理可辐射到其他小batch size上，但是同样也有好处，可以在达到一定程度后用来精细的去磨一下你的模型。因为小批量在学习过程中加入了噪声，会有一些正则化效果，但是需要小的学习率保持稳定性，否则还是会跳进局部极值难以自拔 对于大batch size，达到相同精度需要更多epoch，因为每轮训练中的迭代次数更少。 所以可以总结下，适当增大batch size好处在于： 内存利用率提高了。 对于相同数据量的处理速度更快。 在一定范围内，一般来说batch size 越大，其确定的下降方向越准，引起训练震荡越小。 但是盲目增大绝对不行，毕竟首先不是所有实验室都有那么好的设备，batch size上去了，首先也得有足够内存和显存支撑。 另外根据上面提到的要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢。主要还有一点，batch size大到一定程度，下降方向已经基本不再变化，加了也没什么用。 所以说本质上还是需要考虑到你的任务到底塞多少合适，另外你的设备到底能怎么撑。一般来说，取2,4,8这么多块卡并行跑，选择一个合适的batch size塞显存就很重要了，因为要考虑显存占用率，你的实验室基友们也不是不跑实验的……不过一般来说，开始训勇敢取个大batch并没啥坏处，有bn撑腰，一切显得美好很多。 另一方面普遍要动的超参就是学习率了。嗯……这没啥好说的，为了训练稳定自然都愿意选小一点的lr或是先大lr降，不收敛不稳定再换小lr跳出震荡。其他自引入超参，emmmm，仁者见仁智者见智，其实是闻者伤心听者流泪……都是调参血泪史。 当然了，上面说的一切都要看你的实验室/公司项目组有没有钱，毕竟钱才是科学第一推动力。感谢以前搭集群的师兄们，真的，实属带善人。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[反向飞驰]]></title>
    <url>%2F2018%2F09%2F09%2Frun-backward%2F</url>
    <content type="text"><![CDATA[最近事情堆积如山，准备cvpr的进一步实验，开始校招的长周期准备。其实投稿大概就是督促自己做点事情，毕竟一直只是实验不带着压力工作效率就不高。 前途非常迷，今年校招的行情比较迷，看不懂，没有一篇顶会文章算法岗做敲门砖，算法岗很难进大厂，本身我在研究上也不擅长。准备拿起老本行，准备找开发，负面情绪堆积严重，毕竟和以前赶上好时候的人相比容易心态爆炸，没有比较就没有伤害。 其实大部分时间我对于成功和开心的定义都比别人简单： 波澜不惊就算开心 比普通人过的好一点就算成功 然而最近基本都是反向飞驰。没有一天安稳，没有一天看到超越平庸的曙光。 急需调整心态，明天开始扑在framework修改和刷题。]]></content>
      <categories>
        <category>杂谈</category>
      </categories>
      <tags>
        <tag>杂谈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch升级和imread区别]]></title>
    <url>%2F2018%2F09%2F01%2Fiocv%2F</url>
    <content type="text"><![CDATA[前段时间写了一版multiposenet的结构，调试的时候发现了几个问题。 首先是pytorch0.4.0到0.4.1的更新。这个版本的更新里引入了一个比较有用的降采样函数，torch.nn.Functional.interpolate。文档上介绍的也很明确，调用时可以指定升/降采样的size或者一个尺度因子scale_factor。之前其实一直没有一个官方给出的降采样函数或是层，原来融合多个size每次都要复用手写的层，某些时候还不知道对不对的上别人文章的降采样方法……这回添加了一个官方的降采样方法，这样可能细节上可能大家都能统一一点，实验代码也好写一点或是复现一点，而且可以下采样到任意size或是scale，避免了很多方法实现上的微妙区别。load模型的时候跟0.4.0没什么区别，不像0.3.0升0.4.0时由于upsample变动导致我大量模型load不能的囧境。 另外就是关于skimage.io.imread和cv2.imread的一个细节问题，读灰度图时，skimage的读取会读出2通道，而opencv中不添加灰度的flag的时候传入会默认处理成三通道。一开始我随手拿io.imread读了原图，原灰度图是这个样子： 训练跑到一半的时候突然发现tensor运算的时候左右值维度不匹配，才发现了这个细节问题……之前一直都是用cv2.imread,也不会往里传grey的相关参数，直接默认读出三通道，但是灰度图直接拿io.imread调用时会出现shape只有(width,height)两位的问题。去翻了翻skimage的源码，as_grey参数已经deprecated了，其中有这么一段处理：1234567if img.ndim &gt; 2: if img.shape[-1] not in (3, 4) and img.shape[-3] in (3, 4): img = np.swapaxes(img, -1, -3) img = np.swapaxes(img, -2, -3) if as_gray: img = rgb2gray(img) 内部在ndim大于3的情况下，flag设为True也只是会粗暴的直接转化，ndim为2的时候他就直接读出(height,width)维的数组了，所以说这个确实……cv的读取就避免了这个问题。看来还好以前都是直接调cv.imread，这次因为要调transform顺手调了io，以后还是得注意数据喂进去到底是啥……以前也出现过喂错东西跑瞎实验的情况，毕竟数据驱动型的工作，数据的纯净度、形式组织还是至关重要的，得涨点记性。 另外就是multiposenet的子网络问题，目前我也不知道作者在test-dev上的统一size到底用了多大。而且按照原文中的结构，不给出一些细节，最后的ground truth和heatmap的size是绝对不匹配的，目前我强制采样到了256x256，有理解的朋友可以交流一下，代码repo在这：https://github.com/IcewineChen/pytorch-MultiPoseNet]]></content>
      <categories>
        <category>细节</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[新评论gitalk系统踩坑]]></title>
    <url>%2F2018%2F08%2F30%2Fgitment%E2%80%94%E2%80%94config%2F</url>
    <content type="text"><![CDATA[放弃issue形式的评论了。已经在新blog启用valine了，issue有更新会比较烦人。 选择过程现在博客已经改成valine了，对一些不常用github的访客来说，评论更简单，如果想用gitalk，可以参照本文。 网易云跟帖和多说都挂掉了，所以急需一款新的比较好用的跟帖评论工具。比较了下面几种： hexo官方文档里的扩展：disqus，因为墙你懂得 来必力 同样存在加载慢的问题 gitalk 看起来不错，依托github issue, 支持markdown hypercomments，唯一缺点不支持markdown 好吧那就选gitalk吧 实际工作其实按照网上多如牛毛的添加gitalk教程来就行，我随手在这贴一个：https://www.jianshu.com/p/9be29ed2f4b7 ，感谢这位的分享，虽然我大概看的不是他的分享……只是表达一下随手就能搜出一斤的情况。 那说好的坑呢？ 别急……有俩坑。第一个坑在github应用创建的时候，如果你绑定了自己的个性化域名，homepage url和回调url一定填自己绑定后的域名。这个我开始绑定的github.io，回调就报错。 第二个坑就出在网上花花绿绿不同教程上了。主要是gitalk配置字段的问题。不论你选择哪一种方法，都会有在主题配置文件添加字段的选项，如下：123456789# Gitalkgitalk: enable: true clientID: 'your clientID' clientSecret: 'your clientSecret' repo: Blog_comments owner: erbiduo admin: erbiduo distractionFreeMode: true 12345678gitalk: enable: true githubID: github帐号 # 例：asdfv1929 repo: 仓库名称 # 例：asdfv1929.github.io ClientID: Client ID ClientSecret: Client Secret adminUser: github帐号 #指定可初始化评论账户 distractionFreeMode: true 这是我在网上找的两个版本的主题文件配置内容，你猜哪个是对的？ 这就需要看你在themes/next/layout/_third-party/comments/gitalk.swig内的字段怎么写的了，swig文件添加进去的内容：12345678910111213141516&#123;% if page.comments &amp;&amp; theme.gitalk.enable %&#125; &lt;link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css"&gt; &lt;script src="https://unpkg.com/gitalk/dist/gitalk.min.js"&gt;&lt;/script&gt; &lt;script type="text/javascript"&gt; var gitalk = new Gitalk(&#123; clientID: '&#123;&#123; theme.gitalk.ClientID &#125;&#125;', clientSecret: '&#123;&#123; theme.gitalk.ClientSecret &#125;&#125;', repo: '&#123;&#123; theme.gitalk.repo &#125;&#125;', owner: '&#123;&#123; theme.gitalk.githubID &#125;&#125;', admin: ['&#123;&#123; theme.gitalk.adminUser &#125;&#125;'], id: location.pathname, distractionFreeMode: '&#123;&#123; theme.gitalk.distractionFreeMode &#125;&#125;' &#125;) gitalk.render('gitalk-container') &lt;/script&gt;&#123;% endif %&#125; 很明显，如果在部署的时候解析你主题文件的配置，模板字段里Gitalk对象肯定要和配置文件里字段一致，也就是说你的配置文件里gitalk下其他字段要跟Gitalk对象内的字段一致，gitalk.ClientID对应在主题配置文件下的就是ClientID字段，owner:gitalk.githubID和主题配置下githubID字段要这样一一对应，我按照某些教程搭建的时候就出了问题，调试了一下找到了这俩坑。其他的基本就按教程配置就行，最后注意swig中字段和theme下的config.yml一致就可以。]]></content>
      <categories>
        <category>技术杂谈</category>
      </categories>
      <tags>
        <tag>杂谈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[教师论]]></title>
    <url>%2F2018%2F08%2F30%2F%E6%95%99%E5%B8%88%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[缺德请挨打。 https://www.mysupervisor.org/viewtopic.php?f=1096&amp;t=50671&amp;sid=18f3512f1b806b030a741a6746af0969]]></content>
      <tags>
        <tag>杂谈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pose estimation tutorials]]></title>
    <url>%2F2018%2F08%2F19%2F%E5%88%86%E4%BA%AB%2F</url>
    <content type="text"><![CDATA[分享一版整理的一些姿态估计心得的repo：https://github.com/IcewineChen/Pose-estimation_tutorials]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[谈谈gluon和其他我用过的框架]]></title>
    <url>%2F2018%2F07%2F29%2Fgluon%2F</url>
    <content type="text"><![CDATA[关于gluon前几天跟小伙伴们讨论到了几个问题，突然讨论这个事情：比如说我的数据load进来之后，想把不同的batch并行，这样该怎么弄？ 我很喜欢pytorch，但是我大部分时间对pytorch的并行调用dataparallel时的实际操作是把数据一个batch拆分，然后并行到设备上，很明显不符合需求，要是改动还得对底层动刀。然后突然想到嗯，好像除了这个问题，有些时候去符号式这种东西，还真不太适合快速复现或者验证想法。目前大部分时间复现我确实都优先考虑mxnet做。如果不太需要自定义层，基本才会考虑pytorch。然后呢……就是安利一发mxnet了。顺带也相当于总结下我对这些框架的使用体验。最早在16年的时候，我猜大家肯定都在耍……caffe。那时候出于我写c++写习惯了的，一度出现了c++是世界上最好的语言的错觉……事实上caffe确实定制度好，自由度高，python接口也问题不大，但是确实需要自定义的太多，编译各种问题吐血，而且出于语言习惯，我更乐意从头写个layer，backward慢慢磨，然而这确实相当浪费时间。而且到后来越来越多的文章都在用tensorflow，出于代码的编写和调试时间成本，还是基本放弃了caffe，除非有时候看一些早期的文章，放出的代码基于caffe还会看一看，其他时候基本不考虑caffe了。caffe2出了以后，detectron似乎也没能完全拯救他，我是编译了一份扔一边了。不过当年在公司实习的时候，大家确实也都在用caffe，我的结论就是是出于部署和性能考量，caffe比较适合。然而mxnet在分布式和部署上我觉得不比caffe差，事实也确实是后续业务也开始转向mxnet。总体而言caffe在科研上开始没用tensorflow或是pytorch那么闪光了。 之后tensorflow。没什么可多说的，基本上你肯定得会。放tensorflow代码的文章多如牛毛，文章多，社区好，早期时候带自动微分这种牛逼功能，背景牛逼，而且符号式编程其实更符合我的思维方式（也可能有人对符号式编程写model不习惯，这个只是个人意见）。然后绝大部分时间我research绝不会写tensorflow的代码。为啥……那个调试真的痛苦的要死，我对tensorflow的更新最终停在了estimator出来的那个版本。虽然后期的eager Execution机制引入动态图，但是本质上依然只是一个功能，整个框架依然是基于静态图开发的。tensorflow对于实现你表达的思路是个非常非常优秀的框架，然而确实会给你中间的一些验证带来很多不必要的麻烦，而且相对比较笨重。对于科研实验来说，很多东西是相当冗余的，对于要求快速实验验证想法的research来说这个问题还是比较致命的。 接下来才是比较重要的比较。我觉得对于大部分来说大概还是偏好pytorch和mxnet一点（其实并没mxnet什么毛关系）。支持动态图，当时刚转到pytorch上的我觉得这简直是最棒的机制了。动态图在调试上优势太大，中间变量查看十分十分方便，对于快速验证模型的一些问题提，比如说中间shape，dtype以及各种问题供了巨大便利，不需要静态图的各种包装以及session去跑。在同样采用动态图的基础上，mxnet和pytorch的核心区别，我觉得大概就在符号编程上了。pytorch继承了torch当年的优良意志：我们绝对不用符号式编程……符号式编程简单来说就是把变量就定义成一个符号，之后把数据扔进来的时候再通过name告诉他哪喂什么就好，相对来说很适合直接扔一个symbol在这，最后再考虑数据的问题就好，可以把模型定义单独作为一件事情。而pytorch数据流向直接定义调用就可以，最后需要求导记得用variable封装好即可。 然而gluon出来以后，我觉得mxnet确实兼顾了各家之长。gluon里两点让我觉得特别优秀。第一点就是不得不提的混合编程，这点看下面代码就知道：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253class Hourglass(gluon.nn.HybridBlock): def __init__(self,n,nModules,nFeats): super(Hourglass,self).__init__() self.n = n self.nModules = nModules self.nFeats = nFeats self.up1_ = gluon.nn.HybridSequential() self.low1_ = gluon.nn.HybridSequential() self.low2_ = gluon.nn.HybridSequential() self.low3_ = gluon.nn.HybridSequential() for i in range(self.nModules): self.up1_.add(Residual(in_channels=self.nFeats,out_channels=self.nFeats)) self.low1 = gluon.nn.MaxPool2D(pool_size=(2,2),strides=(2,2)) for i in range(self.nModules): self.low1_.add(Residual(in_channels=self.nFeats,out_channels=self.nFeats)) if self.n &gt; 1: self.low2 = Hourglass(n=n-1,nModules=self.nModules,nFeats = self.nFeats) else: for i in range(self.nModules): self.low2_.add(Residual(in_channels=nFeats,out_channels=nFeats)) for i in range(self.nModules): self.low3_.add(Residual(in_channels=self.nFeats,out_channels=nFeats)) self.up2 = gluon.nn.Conv2DTranspose(channels=self.nFeats,kernel_size=(2,2),strides=(2,2)) def hybrid_forward(self,F,x): up1 = x for i in range(self.nModules): up1 = self.up1_[i](up1) low1 = self.low1(x) for i in range(self.nModules): low1 = self.low1_[i](low1) if self.n &gt; 1: low2 = self.low2(low1) else: low2 = low1 for i in range(self.nModules): low2 = self.low2_[i](low2) low3 = low2 for i in range(self.nModules): low3 = self.low3_[i](low3) # gluon的nn里没有upsample，混合直接调用ndarray的sample # up2 = F.UpSampling(low3,sample_type='bilinear',scale=2) up2 = self.up2(low3) return up1 + up2 这是我定义的一个Hourglass结构，只需要你的net或者模块继承hybridblock，forward时候就需要实现一个hybrid_forward(self,F,x)。这个F是区别于非混合式forward的量，调用时候F.function就会自动找你的function从什么里调用，是从符号式还是正常数值计算的ndarray的包里去找。这个功能真的相当人性，一部分ndarray中实现的函数，在symbol包内或是gluon.nn包内是没有的，这样使得mxnet的可用性和使用体验更好，用户大概只需要关注结构定义就可以。当然逻辑比较简单的网络可以直接Sequential add即可。 二是几乎没什么学习成本。这个结构看着熟不熟悉？基本就和pytorch里的结构定义是一样的。有大量的人都在主用pytorch，这样的风格从pytorch迁移代码十分方便。比起mxnet symbol相对更繁琐的定义，这种class内部定义层，再在forward连接的方式大家确实写起来十分习惯，而且也比较符合我脑子里想像的dl定义范式和思维习惯。mxnet目前唯一的问题大概api更新太快，之前每一版更新变动都不小，还处在高速发展，其实也就是不太稳定的阶段，但是bug也已经减少了很多，可用性高而且复现好用，个人觉得框架选择的话，pytorch最轻松上手，mxnet大概是使用起来感觉最好的。如果硬要选框架自己写代码，主役pytorch或是mxnet都是好选择，然而考虑到科研中的泛用度，mxnet确实略逊pytorch一筹，mxnet代码的文章不是很多。但是就个人喜好而言，mxnet在gluon引入以后可能更好一点。]]></content>
      <categories>
        <category>mxnet</category>
      </categories>
      <tags>
        <tag>mxnet</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实验代码分析]]></title>
    <url>%2F2018%2F07%2F16%2Fhg-3d%2F</url>
    <content type="text"><![CDATA[上次组会分享了CVPR2018的3d pose估计文章 3D Human Pose Estimation in the Wild by Adversarial Learning,文章中提到一种用GAN的方式生成3D in the wild估计的方法。最近准备复现，顺带在这里先分析一下结构中Generator，顺带分析一下代码 生成器的结构参见Towards 3D Human Pose Estimation in the Wild: a Weakly-supervised Approach，感谢作者，该结构以stacked hourglass为base network做一个2d keypoint估计，然后扔进3d regression得到最后的结果。数据集选用了有3d ground truth和没有ground truth的in the wild 2d集拼合，整体结构上在2d估计中抽出了中间层信息送入回归器，迁移了in the wild的信息。 pytorch网络代码分析先贴一段核心网络结构的代码：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980class HourglassNet3D(nn.Module): def __init__(self, nStack, nModules, nFeats, nRegModules): super(HourglassNet3D, self).__init__() self.nStack = nStack self.nModules = nModules self.nFeats = nFeats self.nRegModules = nRegModules self.conv1_ = nn.Conv2d(3, 64, bias = True, kernel_size = 7, stride = 2, padding = 3) self.bn1 = nn.BatchNorm2d(64) self.relu = nn.ReLU(inplace = True) self.r1 = Residual(64, int(128)) self.maxpool = nn.MaxPool2d(kernel_size = 2, stride = 2) self.r4 = Residual(128, 128) self.r5 = Residual(128, self.nFeats) _hourglass, _Residual, _lin_, _tmpOut, _ll_, _tmpOut_, _reg_ = [], [], [], [], [], [], [] for i in range(self.nStack): # n阶的hourglass _hourglass.append(Hourglass(4, self.nModules, self.nFeats)) for j in range(self.nModules): # 连接stacks间的基本残差单元 _Residual.append(Residual(self.nFeats, self.nFeats)) lin = nn.Sequential(nn.Conv2d(self.nFeats, self.nFeats, bias = True, kernel_size = 1, stride = 1), nn.BatchNorm2d(self.nFeats), self.relu) _lin_.append(lin) _tmpOut.append(nn.Conv2d(self.nFeats, ref.nJoints, bias = True, kernel_size = 1, stride = 1)) _ll_.append(nn.Conv2d(self.nFeats, self.nFeats, bias = True, kernel_size = 1, stride = 1)) _tmpOut_.append(nn.Conv2d(ref.nJoints, self.nFeats, bias = True, kernel_size = 1, stride = 1)) for i in range(4): for j in range(self.nRegModules): _reg_.append(Residual(self.nFeats, self.nFeats)) # append部分对应stackhourglass中的沙漏型结构，将其中用的residual模块和n个Stack整理进sequential的形式方便之后再循环里调用 # modulelist的取法其实基本跟list一样，封装在modulelist里会检测到每一个的hook，直接包装在list里，每一个单元内部的参数会初始不到 self.hourglass = nn.ModuleList(_hourglass) self.Residual = nn.ModuleList(_Residual) self.lin_ = nn.ModuleList(_lin_) self.tmpOut = nn.ModuleList(_tmpOut) self.ll_ = nn.ModuleList(_ll_) self.tmpOut_ = nn.ModuleList(_tmpOut_) self.reg_ = nn.ModuleList(_reg_) self.reg = nn.Linear(4 * 4 * self.nFeats, ref.nJoints) def forward(self, x): # 将定义了的层按顺序传递，构建整个网络的forward顺序 x = self.conv1_(x) x = self.bn1(x) x = self.relu(x) x = self.r1(x) x = self.maxpool(x) x = self.r4(x) x = self.r5(x) out = [] for i in range(self.nStack): hg = self.hourglass[i](x) ll = hg for j in range(self.nModules): ll = self.Residual[i * self.nModules + j](ll) ll = self.lin_[i](ll) tmpOut = self.tmpOut[i](ll) out.append(tmpOut) ll_ = self.ll_[i](ll) tmpOut_ = self.tmpOut_[i](tmpOut) # 取多层信息，作为下一个hourglass的输入，也保存信息到送入reg的x里 x = x + ll_ + tmpOut_ for i in range(4): for j in range(self.nRegModules): x = self.reg_[i * self.nRegModules + j](x) x = self.maxpool(x) x = x.view(x.size(0), -1) reg = self.reg(x) out.append(reg) return out 这就是3d stacked hourglass结构的代码。关于一个nn.Module的代码怎么写，其实参照pytorch源码的Module写就可以，由于自动微分这种东西的存在，继承Module时需要只定义init和forward函数即可。在init函数中，我们定义需要的层，需要的结构，然后在forward中，定义层的连接，也就是网络的结构。这段代码描述了生成器的结构，总体来说，nStack就是单个stacked hourglass的阶数，modules是使用级联的模块数，residual是一个残差结构。ll是我们一开始抽出来的stacked的输出层和中间层的和，也就是抽出层再加上输出的结果，之后在输入维度上做整合。reg是3d信息回归器的结构。在forward中，按照stacked hourglass的方法先加了conv+bn+relu+残差+max pooling的结构调整网络输入，之后搭建2d的级联结构，同时把每个stacked结构中的中间一层抽出来,和每个级联的stack一起送去3d regression。具体的代码注释和一些修改我都放在了我fork的repository里，欢迎来看整个注释和代码啊，现在还没彻底完成，还在这个的基础上加入GAN部分的代码（主要是multi-source的鉴别器）去复现GAN-hg-3d。准备在这个结构上调整，看看in the wild的3d pose的效果如何。大概觉得这个方向在工业界落地上可能还有点价值……以前实习的时候组里也有人做过2d的gesture，问题本质上都差不多。理论上行为这一块市场很大，pose的工作还是很有价值的。 pytorch在大部分时间是不需要定义backward的，基本上直接继承nn.Module就可以实现。不过有时候需要我们自定义loss，这时候loss从function里继承，这里其实本质上你也可以直接写在新的Module里，但是你可以尝试一下在Module的backward里面加上一些print，之后从外部调一下试试看。这时候是不会执行的。因为本身Module底层就是要进行backward的操作，每一步操作的时候都会生成一个Function的子类，最后把这些子类的forward和backward连接到一起。所以这些时候你在module内部写一个backward它根本就不会执行，因为实际上真的调用的是基于Function的backward操作。所以如果有大量自定义而pytorch的底层又没有实现的计算，可以直接自定义一个继承自Function的class，然后再用module封装进去。这也是官方文档的做法，在neural style transfer的那篇tutorial里很好的表示了loss的写法，想再自己动手试试看看的话可以去看一这篇文档。根据我的经验你大概只需要记住：绝大部分时候你不需要自己写backward，但是比如你要实现一种新的下降算法或是已经提供的计算不能满足的层定义，那再自己去写。 那么如果自定义一些操作，请遵照一下结构：123456789class CustomLoss(torch.autograd.Function): def __init__(self,*kwargs): super(CustomLoss,self).__init__() def forward(self,*kwargs): raise NotImplementedError def backward(self,) raise NotImplementedError 按照function继承是要定义backward的，对grad要进行底层实现，基本上只有loss这部分的定义需要费点事。mxnet的定义倒是可以根据符号编程自动推导，符号式编程加自动微分确实很方便，不过sym的调试其实并没有pytorch的动态图来的直观。感觉mxnet在复现上还是更简单快捷粗暴，不过文档不全和有些地方因为更新太快有bug，稍微有点脱了后腿。pytorch这次的更新据说填了不少坑,就我的感受来说，内存不那么容易被多输入Variable的索引跑到炸了，之前的话以Variable对接dataloader在梯度更新后经常要注意取loss[0]，否则直接会加入Variable中的各种参数。据说目前版本解决了这个问题。 数据集接口没什么太多问题的，dataloader只需要注意cv读和dataloader要求的维度顺序，还有定义最后混合的fusion结构中要构建包含2d ground truth和3d ground truth的样本即可。getitem基本不需要做多少改动，只需要按顺序，3d+2d顺序append，之后索引在3d样本数内读3d，大于部分get 2d样本即可。 关于版本更新导致的问题其实原repo的作者是给出了pretrain的模型的，但是在Stage2阶段是用不了的……我单点打到这一行，发现load的模型是有问题的，这一行定在upsample层上： self.up2 = nn.Upsample(scale_factor = 2) 事实上upsample需要一个align_corners的参数，在0.4.0之前这个参数应该被直接给了None，导致在0.4.0版本load pretrain的模型后，跑到upsample层的forward时在module的检查时一直报错，会一直报给你检查不到align_corners。再打进内部检验的断点时，发现确实load的模型align_corner是None。如果使用我修改后的pytorch0.4.0兼容的代码train的模型是不存在这个问题的，然而我检查的时候依然没有找到align_corner参数，理论上应该default是False。这可能是底层实现有改动，更底层的部分我就step out没管了。这个大概是版本更新导致的bug，具体有待继续调试验证。如果想看pytorch修改后能用的代码，可以直接跑我fork后的fixed branch，里面已经做了对应的各种修改，在python2.7+ubuntu16.04+pytorch0.4.0下可以正常运行。 后续工作之后会打算对文中内容复现。不过目前遇到一个问题是关节点depth map怎么得到。这个问题目前没找到什么合适的代码或是工具，如果有大佬知道的话希望能邮件或是github issue给我……非常感谢]]></content>
      <categories>
        <category>deep learning</category>
        <category>paper reading</category>
        <category>pose estimation</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>pytorch</tag>
        <tag>human pose estimation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[没有任何意义的高亮测试]]></title>
    <url>%2F2018%2F07%2F13%2F%E9%AB%98%E4%BA%AE%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[123456#include &lt;iostream&gt;using namespace std;int main(void)&#123; cout &lt;&lt; "highlight" &lt;&lt; endl;&#125; 意义不明的测试之前想写多写点代码分析和最近跑的实验代码的剖析，结果发现代码高亮不了……现在hexo的代码高亮卡的比较严，首先得先把site下的config里的highlight字段的几项变为true（之前的auto_detect是false），再把主题下的config里的highlight的主题修改成自己想修改的即可，hexo里自带了5种高亮。 下一篇填坑=-=之后周末填个坑吧……pytorch0.4其实最近跑了一下好多坑都被填完了。周末随便写点mxnet和pytorch的代码分析和总结吧。就酱。想回家睡两觉。不过想先把最近看的番总结一下，顺带给island安利一波……非常超神的游戏，可能这两天给今年一月番做个总结，讲讲游戏，开个游戏块吧……这个博客大概以后会频繁更更，也当发发牢骚了，顺带在漫评路上走一走……多说和网易云跟帖都挂了，所以评论暂时就不开了]]></content>
      <categories>
        <category>杂谈</category>
      </categories>
      <tags>
        <tag>杂谈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人体姿态估计方法综述]]></title>
    <url>%2F2018%2F07%2F08%2Fhuman-pose-estimation%2F</url>
    <content type="text"><![CDATA[从上学期末到现在，在姿态估计方面的工程和论文、实验也算做了不少。现在想着总结一下，理清一下思路，对之后的研究和实验也能提供一点帮助。pose estimation是个很有意思的课题，目前2d的pose已经做得很成熟了，in the wild或是多人任务都已经非常不错了，3d pose目前还有很多潜力可挖。2d pose多用heatmap特征，ground truth多以keypoints的ground truth坐标高斯分布内生成。3d上方法普遍还是以2d的结果+深度信息的回归做框架。不过最近读了一篇CVPR2018的3d pose工作，觉得3d pose在in the wild上还有很大潜力可挖，在这里也会分享一下。 研究概述2d pose estimation从CVPR2016到AAAI2018，2D &amp; 3D human pose estimation（姿态估计）浮现了不少好的方法。按时间顺序来说，我大概印象比较深刻的有： Convolution pose machine，出自2016的CVPR。论文地址(https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Wei_Convolutional_Pose_Machines_CVPR_2016_paper.pdf) ，CMU的工作，一种2D pose估计的方法，利用多个不同scale的effective Receptive Field对应的heatmap进行姿态估计。我个人觉得这篇工作其实很不错，之前做的项目用到过cpm的model做预测，对于固定摄像角度的单人姿态估计非常不错，不过model嵌入到系统里速度和效果不如下面提到的openpose，而且工作在2018年视角看来也比较老了……不过我觉得CPM工作挺经典的，我最早接触的工作也是这篇。 Stacked Hourglass Network，论文地址（https://arxiv.org/abs/1603.06937) ，非常经典的工作，很多后续的工作都以stacked hourglass作为backbone或者baseline，许多3d pose的工作也借鉴了这一结构。这篇文章提出的stacked hourglass的方法，用了一种downsample-upsample的encode-decode的结构。stacked hourglass使用了一种residual module的初级模块，采用了卷积路加跳连路，跳连路卷积核保证单元输入输出channel相同。之后以residual module为base结构，逐级降采样后再重新升采样，每一个降采样路会跳出一个跳连路以保留原尺度，再与升采样后同一尺度的输出相加，整个结构图如下所示 鼎鼎大名的openpose库的文章，出自2017的CVPR。Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields，也是来自CMU的工作。这个work工程价值也很高，在github上也开源了openpose库，自带demo里有一个多人场景下的pose estimation demo。这篇文章和工作主要贡献在提供了一种多人场景下的姿态估计方法，不同于许多先做检测再做pose的自上而下方法，这篇文章直接采用了自下而上的思路，将关键点heatmap和人的关节之间的连接策略一起得到。base network采用了一种multi-branch的结构，其中一路用于提keypoint的confidence map，另一个branch用于提PAF，也就是用于关节连接的特征，每一路都有多个stage，最后自然而然地实现了多人场景下的pose estimation。这篇文章的网络结构如下所示 pyramid residual network，出自ICCV2017的文章。 文章全名learning feature pyramids for human pose estimation。可以直接去ICCV2017的库里搜。这篇文章好在写的非常详细……怎么说呢，就是读起来非常非常舒服，文章结构很好。其实核心思想就是在stacked hourglass based network的基础上，将residual module替换成了pyramid module，pyramid residual module利用了输入的多个scale，在性能上得到了一定的提升。文章还提到了这种multi-scale的网络初始化的一些trick，并且设计了一个实验来验证residual network中将shortcut一路改成BN+relu+3x3卷积效果更好（虽然我实在想不明白为什么）……不过我觉得这文章的写法和思路非常清晰，所以拿出来想说一下。下面给出prm单元的结构，就是这种利用了输入的多个scale的multi结构，也算是比较巧妙 3d pose estimation接下来想提两篇3d pose estimation的工作。也可以说是两篇半……因为有一篇其实是听组内分享时候学习的=-= 首先简单提下这半篇吧。出自A simple yet effective baseline for 3d human pose estimation。以2d的骨架作为输入，然后回归直接得3d骨架信息。整个结构如下所示。延续了2d+regression的思路。 这周末读了两篇in the wild的3d pose工作。第一篇是Towards 3D Human Pose Estimation in theWild: aWeakly-supervised Approach， ICCV2017的工作。这篇文章利用了一种弱监督的思路，将2d和3d dataset放在一起训练，利用一些先验经验定义了一个2d数据集生成结果的3d 回归loss。该方法的数据集combine了2d和3d集，3d集有3d pose的ground truth，但是3d数据集都是出自lab环境，起不到in the wild的无约束效果。那怎么办？引入2d集做弱监督。整体网络结构还是先用stacked hourglass做2d预测，不同的是这次利用了stacked hourglass中多个层的信息作为3d回归模块的输入，再去进行3d回归，这样在3d回归的模型中，引入的in the wild的2d数据集的弱监督就会将constraint引入了结果，实现了in the wild上的性能提升，我在这理解为一种transfer的trick。这个思路也在后面的基于GAN的工作中作为generator。整体结构如下 之后是我这周末读的另一篇工作，跟PRM出自同一个组的工作，3D Human Pose Estimation in the Wild by Adversarial Learning，出自CVPR2018。整体思路是用GAN改善一下上面这篇的work。generator直接选用了上一篇的结构，在discriminator上，用了一种多输入的结构： 在其中，geometric descriptor是将ground truth和sample的x,y,z坐标之差和平方直接取pairwise形式的6D向量，2d heatmap和depth map直接就是从2d和3d module得到，这个结构值得借鉴。discriminator将所有3d信息编码后以1为y算一个binary entropy loss，2d sample以0为y去算loss。训练时，先train生成器，再轮流train discriminator和generator即可。 总结到目前来说，pose estimation是一个还算比较热门的课题。目前看来工作相关热度不低，而且还有些能做的地方，比如说3d pose目前的工作就不是很多，2d多人估计的方法还需要进一步改善，目前最好的是sjtu提出的alphapose，不过该库沿用了自顶而下的设计，所以我觉得基于PAF这种连接特征的自底而上的方法还有探究空间。之后不太忙的时候可能把network代码拉出来分析分析再写一篇吧……]]></content>
      <categories>
        <category>deep learning</category>
        <category>paper reading</category>
        <category>pose estimation</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>pose estimation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[batch hard triplet loss]]></title>
    <url>%2F2018%2F07%2F05%2Fbatch-hard%2F</url>
    <content type="text"><![CDATA[I have reproduced a mxnet-version of batch_hard triplet loss.If you want to get the code, click this website(https://github.com/IcewineChen/mxnet-batch_hard_triplet_loss). I achieve the batch hard in this repository. BasedBased on (https://arxiv.org/abs/1703.07737).If you want more details of batch hard triplet loss, read this paper please. It’s really a good work. Introduction In this paper,authors propose a new format of triplet loss called batch hard Efficient for re-id tasks Inplement the hard mining method and soft-margin Can be used in re-id tasks Architecture Used resnetV2 to get 128-dimensions embeddings Used triplet loss to train embeddings 大概意思其实就是我用mxnet复现了一版batch hard triplet loss的代码。最早我是在re-id task相关的文章上看到了这篇方法，原文在arxiv上，17年出的……当时用了作者给的repo跑，实验了batch hard和batch all两种形式的triplet loss，发现效果相当不错，思路写得也相当清晰（具体文章请见上面给的链接哈=-=），不过paper reading太久之前做的了，可能之后再用到这种思路的时候会写一篇paper reading整理思路。不过其实为什么work，文章的思路已经写得很明白了，hard mining。batch-all的方法利用了全部可以使用的三元组，将样本中的三元组利用大幅提高，这种trick非常简单粗暴，但是确实在batch-hard前在market1501的benchmark上非常行之有效。但是batch hard的方法从另一个角度入手，对batch内的样本进行hard-mining，去学习极端的三元组。这个hard-mining的过程简单可以概括为选最难的，也就是对选取的anchor距离最远的正样本，同理再选最难的负样本组成一个三元组，这样就选择了最有效的三元组，收敛后效果一定是比原始的triplet loss效果要好的。这个思路非常的简单易懂，但是确实很work。为了快和方便调试我就做了一版mxnet版本的batch hard的代码。tensorflow版本调试的时候确实比较尴尬……相比之下同样采用符号式编程思想的mxnet调试相当简洁，动态图中间的值可以各种print和查看，而且dataloader也方便调试，继承以后主要就注意两点：一是shape二是变量名，内部的检查机制主要就这两件事，由于值在动态图计算过程中打断点到内部很简单，所以调试起来也非常爽快。如果对这版代码或是相关工作感兴趣，请戳我的github主页，链接戳这里：(https://github.com/IcewineChen/mxnet-batch_hard_triplet_loss)]]></content>
      <categories>
        <category>mxnet</category>
      </categories>
      <tags>
        <tag>mxnet</tag>
        <tag>deep learning</tag>
        <tag>triplet loss</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[caffe安装]]></title>
    <url>%2F2017%2F09%2F04%2Fcaffe-installation%2F</url>
    <content type="text"><![CDATA[关于安装本篇讲述的是关于A卡rmbp的caffe安装与编译中一些问题的解决。 Caffe版本我download下的是目前最新的caffe1.0。关于download caffe1.0的包，直接git clone下来 git clone https://github.com/BVLC/caffe.git 配合食用的工具brew基本都用。没有下载homebrew的盆友请如下操作安装homebrew及依赖项12/usr/bin/ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"brew install wget 之后的caffe编译过程建议选择cmake，cmake使用在后面会说。我们会选用brew进行安装 需要的环境1.python与c++环境准备首先是关于python的问题。之前本地上安装的2.7和3.6，但是在安装过程中改动Makefile的过程中始终会导致numpy连接到System下的pythonframework里。由于这个问题有两个解决方法，后面会详细说道。如果不想释放权限，可以选择安装anaconda，对于之后Makefile.config中的配置比较友好。如果想关掉Rootless对Python.framework中的文件进行编辑，选择进入恢复模式csrutil disabled即可。然而这种头很铁的行为我不推荐。之后会以anaconda配置python的方式便于进行caffe python接口的编译。BAIR的install文档关于libstdc++ installation的部分没什么需要多说的，基本上都不会因为标准库困扰。 2.关于anaconda不能翻的盆友可以直接去tsinghua找一份anaconda的安装包，为了更好的支持caffe1.0，建议选择python2.7版本进行下载。这给一份下载链接： https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda2-5.0.0-MacOSX-x86_64.pkg 之后懒人包安装。对环境变量进行配置。视个人使用在~/.bashrc或~/.zshrc(宇宙第一fish fish~/.config/fish/config.fish去set)中添加 export PATH=~/anaconda/bin:$PATH 之后关于anaconda的使用问题直接直接-h即可，我们直接使用anaconda建立并激活python2.7环境123conda create --name python27 python=2.7activate python27source activate python27 其他包的管理可以选择pip或者conda install进行。 3.其他依赖项安装根据BAIR的官方文档进行安装。1234brew install -vd snappy leveldb gflags glog szip lmdb# need the homebrew science source for OpenCV and hdf5brew tap homebrew/sciencebrew install hdf5 opencv 如果之前安装过snappy等库，卸了更新，写个脚本sh一下，依赖项也这部分可以参见官方文档1234567for x in snappy leveldb gflags glog szip hdf5 lmdb homebrew/science/opencv;do brew uninstall $x; brew install --fresh -vd $x;donebrew install --build-from-source --with-python -vd protobufbrew install --build-from-source -vd boost boost-python caffe编译安装caffe down下来以后，开始caffe的安装和编译。进入的caffe根目录，以我本地为例如下： 之后准备进行caffe的编译。在这里先要根据你的需要动一下Makefile.config。为了开启python接口和选择只用CPU模式，我们需要对原生makefile的一些部分进行修改首先是CPU_ONLY。将line8的cpu_only=1 uncomment掉，改成只用CPU的模式 之后要进行python和matlab接口安装的配置。如果想要开启matlab接口，将下图的matlab部分uncomment掉并根据你的matlab路径进行设置。我开了python接口的配置。python接口给了你两个选择。没有安装anaconda的话你可以选择将你本地的python2.7路径和numpy/core作为python_include的值，但是每次这样做最后都会发现无法import numpy.core.multiarray，print出numpy路径后会发现这里没有链到lib/python而是链到了/System/Library/Framework中的Python.Frameworks中的numpy库，其中numpy的版本不符合caffe1.0的要求。因此要不改掉rootless，用更新的numpy替代。这里有链接关于此问题的解决： https://stackoverflow.com/questions/20518632/importerror-numpy-core-multiarray-failed-to-import 如果不想这么改动，请启用anaconda进行python环境设置。之前有讲到anaconda的环境设置，之后需要在anaconda设置的环境中装好需要的库即可。此时python路径如下图中的anaconda几行配置完成 原生的caffe本身不带build和cmake这些目录。我们进入到caffe根目录后，先建立一个build文件夹并在其下进行编译12mkdir buildcmake .. cmake过后会贴出一些信息，展示caffe的安装依赖项是否已经安装齐和版本是否满足要求，以及你需要安装的接口、以及模式的设置，请仔细阅读，如有缺失项请回到caffe根目录下make clean清除并重新按照上面的一些步骤与说明安装依赖项，并重复上述make过程。 之后需要对两个文件改动,一个是cmakecache.txt,另一个是caffeconfig.cmake。其中关于CPU_ONLY的设置还是off，所以需要改动。 看完配置信息，如果觉得没问题，继续make make all make all之后会提示你pycaffe的信息，之后 make pycaffe okay，整个过程配置完毕，最后结果如下： 之后去examples跑一下caffe的例子就可以了。可以看一下转化lmdb等等数据格式的一些预备知识在进行食用。]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>caffe</tag>
        <tag>MacosX</tag>
      </tags>
  </entry>
</search>
